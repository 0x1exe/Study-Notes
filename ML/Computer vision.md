# DETR: Paper explained

Basic: CNN + Transformer
![[Pasted image 20240306131722.png]]
Transformer for DETR looks like this: 
![[Pasted image 20240306131858.png]]



## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ DETR

DETR —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 3-—Ö —á–∞—Å—Ç–µ–π:

- **CNN-–±—ç–∫–±–æ–Ω** –¥–ª—è **—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è feature map **;
- **Encoder-Decoder —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä**;
- **Feed-Forward Network (FFN)** –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏.

### CNN-–±—ç–∫–±–æ–Ω

–ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –º–æ–¥–µ–ª–∏ —Å–µ–º–µ–π—Å—Ç–≤–∞ ResNet, –∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ baseline backbone ‚Äî **ResNet-50**, –∫–∞–∫ —É–∂–µ –±—ã–ª–æ —Å–∫–∞–∑–∞–Ω–æ –≤—ã—à–µ, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
–ù–∞ –≤—Ö–æ–¥–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º $x \in \mathbb{R}^{3 * H_{0} * W_{0}}$, –∞ –Ω–∞ –≤—ã—Ö–æ–¥–µ ‚Äî —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–æ–º $2048 * H * W; H = \frac{H_{0}}{32}, W = \frac{W_{0}}{32}$.
### Transformer part 1: Encoder

–î–∞–ª–µ–µ –º—ã —É–º–µ–Ω—å—à–∞–µ–º –∫–∞–Ω–∞–ª—å–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤ feature map —Å –ø–æ–º–æ—â—å—é 1D —Å–≤–µ—Ä—Ç–∫–∏ ($2048 ‚Üí d, d < 2048$) –∏ —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –µ–µ –≤ 1D —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, —Å–≤–æ—Ä–∞—á–∏–≤–∞—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å $H * W$.  

–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ 1D –≤–µ–∫—Ç–æ—Ä –Ω—É–∂–Ω–∞ –ø–æ—Ç–æ–º—É, —á—Ç–æ —ç–Ω–∫–æ–¥–µ—Ä –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ü–æ—Å–∫–æ–ª—å–∫—É **MHSA (MultiHead Self-Attention)** —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–µ–Ω –∫ –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞–º –≤–æ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ‚Äî –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–±–∞–≤–∏—Ç—å **—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (spatial positional encoding)**, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–º–æ–∂–µ—Ç —Å–µ—Ç–∏ —É—á–∏—Ç—ã–≤–∞—Ç—å –ø–æ—Ä—è–¥–æ–∫ —Ñ–∏—á –≤ –∫–∞—Ä—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

> **–í–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç:** –∫–∞–∫ –∏ –≤ ViT, —ç–Ω–∫–æ–¥–µ—Ä –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä $N$ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤, —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ Multi-Head Self-Attention –∏ FFN, –≥–¥–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ä–∞–∑–º–µ—Ä–æ–º –≤—ã—Ö–æ–¥–Ω–æ–π.

> **–ï—â–µ –æ–¥–∏–Ω –≤–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç:** –≤ self-attention $Query == Key$, –∞ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫ –Ω–∏–º, –≤–µ–¥—å –º—ã —Ö–æ—Ç–∏–º –¥–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç –¥—Ä—É–≥–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Ç–æ–ª—å–∫–æ content-—á–∞—Å—Ç–∏. –¢–∞–∫–∂–µ, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏ ViT, –º—ã –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–∞–∂–¥—ã–π —Å–ª–æ–π. –≠—Ç–æ –∂–µ –≤–µ—Ä–Ω–æ –∏ –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞!

#### EncoderLayer code

```
import torch
import torch.nn as nn


class TransformerEncoderLayer(nn.Module):
		"""
		–ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è —Å–ª–æ—è —ç–Ω–∫–æ–¥–µ—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.
    """
    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,
                 activation="relu", normalize_before=False):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        # –°–ª–æ–∏ –¥–ª—è FFN
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
				
				# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∏–∑ —Å–ø–∏—Å–∫–∞: [ReLU, GELU, GLU]
        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos

    def forward_post(self,
                     src,
                     src_mask: Optional[Tensor] = None,
                     src_key_padding_mask: Optional[Tensor] = None,
                     pos: Optional[Tensor] = None):
        # Forward –¥–ª—è —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–¥—ë—Ç –ø–æ—Å–ª–µ MHSA –∏ MLP.
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫ Query –∏ Key.
        q = k = self.with_pos_embed(src, pos)
        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,
                              key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src

    def forward_pre(self, src,
                    src_mask: Optional[Tensor] = None,
                    src_key_padding_mask: Optional[Tensor] = None,
                    pos: Optional[Tensor] = None):
        # Forward –¥–ª—è —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–¥—ë—Ç –¥–æ MHSA –∏ MLP.
        src2 = self.norm1(src)
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫ Query –∏ Key.
        q = k = self.with_pos_embed(src2, pos)
        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,
                              key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout1(src2)
        src2 = self.norm2(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))
        src = src + self.dropout2(src2)
        return src

    def forward(self, src,
                src_mask: Optional[Tensor] = None,
                src_key_padding_mask: Optional[Tensor] = None,
                pos: Optional[Tensor] = None):
        if self.normalize_before:
            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)
        return self.forward_post(src, src_mask, src_key_padding_mask, pos)
```
#### Encoder code
```
import torch
import torch.nn as nn


class TransformerEncoder(nn.Module):
		"""
		–ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è —ç–Ω–∫–æ–¥–µ—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.
    """
    def __init__(self, encoder_layer, num_layers, norm=None):
        super().__init__()
        self.layers = _get_clones(encoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm

    def forward(self, src,
                mask: Optional[Tensor] = None,
                src_key_padding_mask: Optional[Tensor] = None,
                pos: Optional[Tensor] = None):
        output = src
				
				# –°—Ç–∞–∫–∞–µ–º N —Ä–∞–∑ —ç–Ω–∫–æ–¥–µ—Ä —Å–ª–æ–∏.
        for layer in self.layers:
            output = layer(output, src_mask=mask,
                           src_key_padding_mask=src_key_padding_mask, pos=pos)

        if self.norm is not None:
            output = self.norm(output)

        return output
```
### Transformer part 2: Decoder

Decoder –≤ DETR —É—Å—Ç—Ä–æ–µ–Ω —Ç–∞–∫ –∂–µ, –∫–∞–∫ –∏ –≤ [–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–µ](https://arxiv.org/pdf/1706.03762.pdf) 2017 –≥–æ–¥–∞. –û–¥–Ω–∞–∫–æ –µ—Å—Ç—å –Ω–µ–±–æ–ª—å—à–æ–µ –æ—Ç–ª–∏—á–∏–µ: DETR –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–∏–Ω–≥ –≤–º–µ—Å—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–≥–æ.

- **–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –¥–µ–∫–æ–¥–∏–Ω–≥**: —Ü–µ–ª–µ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º, –æ—Ç–ø—Ä–∞–≤–ª—è—è —á–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏, –≤–ø–ª–æ—Ç—å –¥–æ –¥–ª–∏–Ω—ã $m$ —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
- 
- **–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–∏–Ω–≥**: —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏–∑–º–µ–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ –∞–ª–≥–æ—Ä–∏—Ç–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø–æ–≤–µ—Ä—Ö –ª—é–±–æ–π –º–æ–¥–µ–ª–∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π. –ê–ª–≥–æ—Ä–∏—Ç–º—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–ª–∏ –±–ª–æ–∫ –∏–∑ $b$ —Ç–æ–∫–µ–Ω–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ: –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (PAD —Ç–æ–∫–µ–Ω—ã) –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É—Ç–æ—á–Ω—è—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é $k$ —à–∞–≥–æ–≤, –ø–æ–∫–∞ –Ω–µ –±—É–¥–µ—Ç –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —É—Å–ª–æ–≤–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏. **–í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å:** $k \leq m$ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏ –æ–±—â–µ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.

–í –¥–µ–∫–æ–¥–µ—Ä–µ —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è **–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ**, –ø—Ä–∏—á–µ–º —Ü–µ–ª—ã—Ö –¥–≤–∞:
1. **Spatial positional encoding** ‚Äî —Ç–∞–∫–æ–µ –∂–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ, –∫–∞–∫ –∏ –≤ —ç–Ω–∫–æ–¥–µ—Ä–µ. –ö–∞–∫ –º—ã —É–∂–µ –≥–æ–≤–æ—Ä–∏–ª–∏ –≤—ã—à–µ, –∞–≤—Ç–æ—Ä—ã —Ö–æ—Ç–µ–ª–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç prior knowledge, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ—Å–µ—Ç –≤ —Å–µ–±–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –∞–Ω–∫–æ—Ä—ã. –ü–æ —Å—É—Ç–∏ –∏—Ö —Ä–æ–ª—å –Ω–∞ —Å–µ–±—è –∏ –≤–∑—è–ª–æ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ. –ü—Ä–∏ —ç—Ç–æ–º —É –Ω–µ–≥–æ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ —Å–º—ã—Å–ª–∞, –∏ –æ–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é —É—á–∏—Ç—Å—è —Å –Ω—É–ª—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±—É—á–µ–Ω–∏–∏. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º—ã –¥–∞–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–µ—Ç–µ–∫—Ç–æ—Ä—É –æ –≤–∑–∞–∏–º–Ω–æ–º —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –Ω–µ –≤–Ω–µ—Å–ª–∏ prior knowledge, –ø–æ—Å–∫–æ–ª—å–∫—É positional encoding ‚Äî –æ–±—É—á–∞–µ–º—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä.
2. **Object queries** –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –≤ —Ö–æ–¥–µ –¥–µ–∫–æ–¥–∏–Ω–≥–∞ –∏ –æ—Ç–≤–µ—á–∞—é—Ç –∑–∞ —Å–±–æ—Ä –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–º –æ–±—ä–µ–∫—Ç–µ –∏–Ω—Ç–µ—Ä–µ—Å–∞ —Å –ø–æ–º–æ—â—å—é —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —ç–Ω–∫–æ–¥–µ—Ä–∞. –ü–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º —Å–ª–æ–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –∫–∞–∫ –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã.

> **–í–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ç–µ–∫—Ü–∏–π —Ä–∞–≤–Ω–æ —á–∏—Å–ª—É object queries.**

**–°—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å:** –≤ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ –¥–µ–∫–æ–¥–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–∞ –≤–∏–¥–∞ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è:

1. **Self-attention** —Å–ª—É–∂–∏—Ç –æ–±–º–µ–Ω—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –º–µ–∂–¥—É object queries. –ö–∞–∫ –∏ –≤ —ç–Ω–∫–æ–¥–µ—Ä–µ, –¥–ª—è V –æ–Ω–∏ –Ω–µ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è.
2. **Cross-attention**. –í —ç—Ç–æ–π —á–∞—Å—Ç–∏ object queries —Å–º–æ—Ç—Ä—è—Ç –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –ø–æ–≥–ª–æ—â–∞—é—Ç –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –í –∫–∞—á–µ—Å—Ç–≤–µ $Q$ –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –≤—ã—Å—Ç—É–ø–∞–µ—Ç —Å—É–º–º–∞ object queries —Å —Å–∞–º–æ–π —Å–æ–±–æ–π –ø–æ—Å–ª–µ MHSA, –∞ –≤–æ—Ç $K$ –∏ $V$ –∑–¥–µ—Å—å –¥—Ä—É–≥–∏–µ ‚Äî —ç—Ç–æ –≤—ã—Ö–æ–¥ —ç–Ω–∫–æ–¥–µ—Ä–∞ —Å positional embedding –∏ –±–µ–∑ –Ω–µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∫–∞–∂–¥—ã–π object query –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –Ω–µ–∫–∏–π [SoftPooling](https://arxiv.org/pdf/2101.00440v3.pdf) —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ñ–∏—á–µ–π –∏–∑ —Ç–µ—Ö –∏–ª–∏ –∏–Ω—ã—Ö —á–∞—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –í –∫–∞–∫–æ–π-—Ç–æ —Å—Ç–µ–ø–µ–Ω–∏ —ç—Ç–æ—Ç –º–æ–¥—É–ª—å –∑–∞–º–µ–Ω—è–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π RoIPooling, —Ç–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç—ã –º–æ–≥—É—Ç —Å—á–∏—Ç—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ –≤—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –∏–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏.


#### DecoderLayer
```
import torch
import torch.nn as nn


class TransformerDecoderLayer(nn.Module):
		"""
		–ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è —Å–ª–æ—è –¥–µ–∫–æ–¥–µ—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.
    """
    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,
                 activation="relu", normalize_before=False):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        # –°–ª–æ–∏ –¥–ª—è FFN
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)
				
				# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∏–∑ —Å–ø–∏—Å–∫–∞: [ReLU, GELU, GLU]
        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos

    def forward_post(self, tgt, memory,
                     tgt_mask: Optional[Tensor] = None,
                     memory_mask: Optional[Tensor] = None,
                     tgt_key_padding_mask: Optional[Tensor] = None,
                     memory_key_padding_mask: Optional[Tensor] = None,
                     pos: Optional[Tensor] = None,
                     query_pos: Optional[Tensor] = None):
        # Forward –¥–ª—è —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–¥—ë—Ç –ø–æ—Å–ª–µ MHSA –∏ MLP.
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫ Query –∏ Key.
        q = k = self.with_pos_embed(tgt, query_pos)
        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,
                              key_padding_mask=tgt_key_padding_mask)[0]
        tgt = tgt + self.dropout1(tgt2)
        tgt = self.norm1(tgt)
        # –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ç–æ, —á—Ç–æ –ø–æ–¥–∞—ë—Ç—Å—è –∫–∞–∫ Q, K, V –≤ attention.
        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),
                                   key=self.with_pos_embed(memory, pos),
                                   value=memory, attn_mask=memory_mask,
                                   key_padding_mask=memory_key_padding_mask)[0]
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt)
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
        tgt = tgt + self.dropout3(tgt2)
        tgt = self.norm3(tgt)
        return tgt

    def forward_pre(self, tgt, memory,
                    tgt_mask: Optional[Tensor] = None,
                    memory_mask: Optional[Tensor] = None,
                    tgt_key_padding_mask: Optional[Tensor] = None,
                    memory_key_padding_mask: Optional[Tensor] = None,
                    pos: Optional[Tensor] = None,
                    query_pos: Optional[Tensor] = None):
        # Forward –¥–ª—è —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–¥—ë—Ç –¥–æ MHSA –∏ MLP.
        tgt2 = self.norm1(tgt)
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫ Query –∏ Key.
        q = k = self.with_pos_embed(tgt2, query_pos)
        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,
                              key_padding_mask=tgt_key_padding_mask)[0]
        tgt = tgt + self.dropout1(tgt2)
        tgt2 = self.norm2(tgt)
        # –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ç–æ, —á—Ç–æ –ø–æ–¥–∞—ë—Ç—Å—è –∫–∞–∫ Q, K, V –≤ attention.
        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),
                                   key=self.with_pos_embed(memory, pos),
                                   value=memory, attn_mask=memory_mask,
                                   key_padding_mask=memory_key_padding_mask)[0]
        tgt = tgt + self.dropout2(tgt2)
        tgt2 = self.norm3(tgt)
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))
        tgt = tgt + self.dropout3(tgt2)
        return tgt

    def forward(self, tgt, memory,
                tgt_mask: Optional[Tensor] = None,
                memory_mask: Optional[Tensor] = None,
                tgt_key_padding_mask: Optional[Tensor] = None,
                memory_key_padding_mask: Optional[Tensor] = None,
                pos: Optional[Tensor] = None,
                query_pos: Optional[Tensor] = None):
        if self.normalize_before:
            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,
                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)
        return self.forward_post(tgt, memory, tgt_mask, memory_mask,
                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)
```
#### Decoder
```
import torch
import torch.nn as nn


class TransformerDecoder(nn.Module):
		"""
		–ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è –¥–µ–∫–æ–¥–µ—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.
    """
    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):
        super().__init__()
        self.layers = _get_clones(decoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm
        self.return_intermediate = return_intermediate

    def forward(self, tgt, memory,
                tgt_mask: Optional[Tensor] = None,
                memory_mask: Optional[Tensor] = None,
                tgt_key_padding_mask: Optional[Tensor] = None,
                memory_key_padding_mask: Optional[Tensor] = None,
                pos: Optional[Tensor] = None,
                query_pos: Optional[Tensor] = None):
        output = tgt

        intermediate = []
				
				# –°—Ç–∞–∫–∞–µ–º M —Ä–∞–∑ –¥–µ–∫–æ–¥–µ—Ä —Å–ª–æ–∏ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∏—Ö –≤ —Å–ø–∏—Å–æ–∫ intermediate.
        for layer in self.layers:
            output = layer(output, memory, tgt_mask=tgt_mask,
                           memory_mask=memory_mask,
                           tgt_key_padding_mask=tgt_key_padding_mask,
                           memory_key_padding_mask=memory_key_padding_mask,
                           pos=pos, query_pos=query_pos)
            if self.return_intermediate:
                intermediate.append(self.norm(output))

        if self.norm is not None:
            output = self.norm(output)
            if self.return_intermediate:
                intermediate.pop()
                intermediate.append(output)

        if self.return_intermediate:
            return torch.stack(intermediate)

        return output.unsqueeze(0)
```




### Transformer: Finale
```
import torch
import torch.nn as nn


class Transformer(nn.Module):
		"""
		–ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.
    """
    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,
                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,
                 activation="relu", normalize_before=False,
                 return_intermediate_dec=False):
        super().__init__()
				
				# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–Ω–∫–æ–¥–µ—Ä.
        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,
                                                dropout, activation, normalize_before)
        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None
        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)
				
				# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–µ–∫–æ–¥–µ—Ä.
        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,
                                                dropout, activation, normalize_before)
        decoder_norm = nn.LayerNorm(d_model)
        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,
                                          return_intermediate=return_intermediate_dec)

        self._reset_parameters()

        self.d_model = d_model
        self.nhead = nhead

    def _reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, src, mask, query_embed, pos_embed):
        # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ NxCxHxW –≤ —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ HWxNxC
        bs, c, h, w = src.shape
        src = src.flatten(2).permute(2, 0, 1)
        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)
        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)
        mask = mask.flatten(1)

        tgt = torch.zeros_like(query_embed)
        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)
        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,
                          pos=pos_embed, query_pos=query_embed)
        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)
```
### FFN

FFN —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —á–∞—Å—Ç–µ–π:

1. **MLP (MultiLayer Perceptron) –±–ª–æ–∫** ‚Äî –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ 3-—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ–µ–≤ —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU. –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ü–µ–Ω—Ç—Ä–∞ –±–æ–∫—Å–∞, –µ–≥–æ –≤—ã—Å–æ—Ç—ã –∏ —à–∏—Ä–∏–Ω—ã.
2. **–õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π**, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∏–π –∫–ª–∞—Å—Å –±–æ–∫—Å–∞ —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ softmax.

–ü–æ—Å–∫–æ–ª—å–∫—É –≤ –∫–æ–Ω—Ü–µ –º—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º $N$ (—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —á–∏—Å–ª–æ; –æ–±—ã—á–Ω–æ –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ, —á–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–∫–æ–º—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏) bbox-–æ–≤, –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å ‚Äúno object‚Äù ‚Äî ‚àÖ. –û–Ω –∏–≥—Ä–∞–µ—Ç —Ç–∞–∫—É—é –∂–µ —Ä–æ–ª—å, —á—Ç–æ –∏ –∫–ª–∞—Å—Å ‚Äúbackground‚Äù –≤ –æ–±—ã—á–Ω—ã—Ö CNN –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞—Ö.
```
import torch
import torch.nn as nn


class MLP(nn.Module):
    """
		–ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è MLP.
    """
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super().__init__()
        # output dim = 4, —Ç–∞–∫ –∫–∞–∫ bbox = [x, y, w, h]
        # num_layers = 3
        self.num_layers = num_layers
        h = [hidden_dim] * (num_layers - 1)
        # –µ—Å–ª–∏ input_dim = hidden_dim = 512, —Ç–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 3-—Ö Linear —Å–ª–æ—ë–≤ - 
	      # (512, 512), (512, 512), (512, 4)
        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)
        return x
```
### DETR
```
import torch
import torch.nn as nn


def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):
	  """
	  –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è.
	  """
    if tensor_list[0].ndim == 3:
        if torchvision._is_tracing():
            # –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è ONNX
            return _onnx_nested_tensor_from_tensor_list(tensor_list)

        # support different-sized images
        max_size = _max_by_axis([list(img.shape) for img in tensor_list])
        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))
        batch_shape = [len(tensor_list)] + max_size
        b, c, h, w = batch_shape
        dtype = tensor_list[0].dtype
        device = tensor_list[0].device
        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)
        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)
        for img, pad_img, m in zip(tensor_list, tensor, mask):
            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)
            m[: img.shape[1], :img.shape[2]] = False
    else:
        raise ValueError('not supported')
    return NestedTensor(tensor, mask)


class DETR(nn.Module):
    """
		–ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è DETR.
    """
    def __init__(self, backbone, transformer, num_classes, num_queries, aux_loss=False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏.
        Parameters:
            backbone: CNN –±—ç–∫–±–æ–Ω.
            transformer: Encoder-Decoder transformer.
            num_queries: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ object queries. —ç—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ
	            –¥–µ—Ç–µ–∫—Ü–∏–π DETR –Ω–∞ –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –î–ª—è COCO –∞–≤—Ç–æ—Ä—ã
	            —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç –±—Ä–∞—Ç—å —á–∏—Å–ª–æ 100.
            
            aux_loss: True –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –ª–æ—Å—Å –¥–ª—è –¥–µ–∫–æ–¥–∏–Ω–≥–∞.
        """
        super().__init__()
        self.num_queries = num_queries
        self.transformer = transformer
        hidden_dim = transformer.d_model
        # num_classes + 1, –ø–æ—Ç–æ–º—É —á—Ç–æ –º—ã –Ω–µ –∑–∞–±—ã–≤–∞–µ–º –ø—Ä–æ no_objects
        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)
        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)
        self.query_embed = nn.Embedding(num_queries, hidden_dim)
        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)
        self.backbone = backbone
        self.aux_loss = aux_loss

    def forward(self, samples: NestedTensor):
        """
        –ù–∞ –≤—Ö–æ–¥ forward –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
        The forward expects a NestedTensor, which consists of:
             - samples.tensor: –±–∞—Ç—á –∫–∞—Ä—Ç–∏–Ω–æ–∫ —Ä–∞–∑–º–µ—Ä–∞ [batch_size x 3 x H x W].
             - samples.mask: –±–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Å–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ [batch_size x H x W],
             —Å–æ–¥–µ—Ä–∂–∞—â–∞—è 1 –Ω–∞ padded –ø–∏–∫—Å–µ–ª—è—Ö.
						
			Forward –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏:
             - "pred_logits": –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –ª–æ–≥–∏—Ç—ã, –≤–∫–ª—é—á–∞—è no-object,
             –¥–ª—è –≤—Å–µ—Ö queries.
              –†–∞–∑–º–µ—Ä = [batch_size x num_queries x (num_classes + 1)]
              
             - "pred_boxes": –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –±–æ–∫—Å–æ–≤ 
             –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç 0 –¥–æ 1) –¥–ª—è –≤—Å–µ—Ö queries —Ä–∞–∑–º–µ—Ä–∞
              (center_x, center_y, height, width).
              
             - "aux_outputs": –µ—Å–ª–∏ aux_loss == True, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è.
        """
        if isinstance(samples, (list, torch.Tensor)):
            samples = nested_tensor_from_tensor_list(samples)
        features, pos = self.backbone(samples)

        src, mask = features[-1].decompose()
        assert mask is not None
        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]
				
				# –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞
        outputs_class = self.class_embed(hs)
        # —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞
        outputs_coord = self.bbox_embed(hs).sigmoid()
        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}
        if self.aux_loss:
            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)
        return out

    @torch.jit.unused
    def _set_aux_loss(self, outputs_class, outputs_coord):
        return [{'pred_logits': a, 'pred_boxes': b}
                for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]

```
## Loss
–û—Å—Ç–∞–ª–æ—Å—å —Ç–æ–ª—å–∫–æ –∫–∞–∫-—Ç–æ —Å–æ–æ—Ç–Ω–µ—Å—Ç–∏ –∏—Ö —Å ground truth –±–æ–∫—Å–∞–º–∏ –∏ –º–µ—Ç–∫–∞–º–∏. –ù–∞ –ø–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥ –º–æ–∂–µ—Ç –ø–æ–∫–∞–∑–∞—Ç—å—Å—è, —á—Ç–æ —ç—Ç–æ –Ω–µ—Å–ª–æ–∂–Ω–æ. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ –≤—Å–µ —Ç–∞–∫ –ø—Ä–æ—Å—Ç–æ: –ø–æ—Ä—è–¥–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ø–æ—Ä—è–¥–∫–æ–º ground truth.

–ö–∞–∫ –∂–µ –∏—Ö —Ç–æ–≥–¥–∞ –º–æ–∂–Ω–æ —Å–º–∞—Ç—á–∏—Ç—å? –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å IoU –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–∏—Ö –±–æ–∫—Å–æ–≤? –ù–æ —Ç–∞–∫–æ–π –º–∞—Ç—á–∏–Ω–≥ —Ç–æ—á–Ω–æ –Ω–µ –±—É–¥–µ—Ç –≤—Å–µ–≥–¥–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º. –ò –∑–¥–µ—Å—å –∫ –Ω–∞–º –Ω–∞ –ø–æ–º–æ—â—å –ø—Ä–∏—Ö–æ–¥–∏—Ç **–∫–æ–º–±–∏–Ω–∞—Ç–æ—Ä–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** ‚Äî –æ–Ω–∞ –æ–±–µ—Å–ø–µ—á–∏—Ç –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ one-to-one –º–∞—Ç—á–∏–Ω–≥–∞, –∫–æ—Ç–æ—Ä—ã–π, –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å, –¥–∞—Å—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω—ã–π —Å—É–º–º–∞—Ä–Ω—ã–π –ª–æ—Å—Å. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **‚Äú–í–µ–Ω–≥–µ—Ä—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º‚Äù (Hungarian algorithm)**, —Ä–∞–±–æ—Ç–∞—é—â–∏–π –∑–∞ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ $O(n^{3})$ –≤—Ä–µ–º—è. –ù–∏–∂–µ –æ –Ω–µ–º –±—É–¥–µ—Ç —Ä–∞—Å—Å–∫–∞–∑–∞–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–µ–µ. –¢–∞–∫–∂–µ —Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ [linear_sum_assignment](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html)¬†–∏–∑ scipy –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–µ —Å–∞–º –≤–µ–Ω–≥–µ—Ä—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º, –∞ –µ–≥–æ –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–∞—è¬†[–º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è](https://ieeexplore.ieee.org/document/7738348).

–í–µ—Ä–Ω–µ–º—Å—è –∫ –ª–æ—Å—Å—É. –û–Ω, –∫–∞–∫ –∏ –≤ –æ–±—ã—á–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞—Ö, —Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –∏–∑ —Å—É–º–º—ã –ª–æ—Å—Å–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏. –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è, L1 –∏¬†[](https://giou.stanford.edu/)[Generalized IoU](https://giou.stanford.edu/). –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (—Ä–∞–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É object queries) –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –±—É–¥–µ—Ç –±–æ–ª—å—à–µ, —á–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∞–ª—å–Ω—ã—Ö ground truth –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ, –ø–æ—ç—Ç–æ–º—É ‚Äú–ª–∏—à–Ω–∏–µ‚Äù –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –≤ –∫–ª–∞—Å—Å ‚Äúno object‚Äù –∏ –Ω–µ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ –∏—Ç–æ–≥–æ–≤–æ–π –Ω–∞–±–æ—Ä –±–æ–∫—Å–æ–≤.
### –í–µ–Ω—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
–ù–∞—á–Ω–µ–º —Å –æ–±—â–µ–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ ‚Äú–∑–∞–¥–∞—á–∏ –æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è—Ö‚Äù, –∫–æ—Ç–æ—Ä—É—é –æ–Ω —Ä–µ—à–∞–µ—Ç:
```
–ò–º–µ–µ—Ç—Å—è –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ —á–∏—Å–ª–æ¬†—Ä–∞–±–æ—Ç¬†–∏ –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ —á–∏—Å–ª–æ¬†–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π. –õ—é–±–æ–π –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞–∑–Ω–∞—á–µ–Ω –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ª—é–±–æ–π (–Ω–æ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–π) —Ä–∞–±–æ—Ç—ã, –Ω–æ —Å –Ω–µ–æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏. –ù—É–∂–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–±–æ—Ç—ã —Ç–∞–∫, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Ä–∞–±–æ—Ç—ã —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏.
```
–ï—Å–ª–∏ –∂–µ –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ –ø—Ä–æ–±–ª–µ–º–µ –º–∞—Ç—á–∏–Ω–≥–∞ –¥–ª—è –ª–æ—Å—Å–∞ –≤ DETR, —Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–∏–¥—É–º–∞—Ç—å, –∫–∞–∫ –ø–æ—Å—á–∏—Ç–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—É —Å—Ç–æ–∏–º–æ—Å—Ç–∏. –í—Å–µ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ: –æ–Ω–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Å—É–º–º—ã (–ø–æ –¥–µ—Ñ–æ–ª—Ç—É –≤—Å–µ –≤–µ—Å–∞ = 1) —Ç—Ä–µ—Ö –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≤—ã—à–µ –ª–æ—Å—Å–æ–≤.

## Downstream tasks

–î–ª—è –∑–∞–¥–∞—á–∏ **panoptic** (–º—ã –µ—â–µ —Ö–æ—Ç–∏–º —Ä–∞–∑–¥–µ–ª—è—Ç—å –æ–±—ä–µ–∫—Ç—ã –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ –∫–ª–∞—Å—Å–∞) —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∞–≤—Ç–æ—Ä—ã –¥–æ–±–∞–≤–∏–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω—É—é –≥–æ–ª–æ–≤—É –∫ DETR, —Ç–∞–∫ –∂–µ, –∫–∞–∫ Faster R-CNN –±—ã–ª —Ä–∞—Å—à–∏—Ä–µ–Ω –¥–æ Mask R-CNN. –î–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º —ç—Ç–æ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ.

- –î–ª—è –Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –±–æ–∫—Å—ã, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ;
- –î–∞–ª–µ–µ –º—ã –ø–æ–ª—É—á–∞–µ–º attention map –∏–∑ MHSA;
- –ú—ã —Ö–æ—Ç–∏–º –º–∞—Å–∫–∏, –ø–æ—ç—Ç–æ–º—É –Ω–∞–∫–∏–¥—ã–≤–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω—É—é –≥–æ–ª–æ–≤—É, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –±–∏–Ω–∞—Ä–Ω—ã–µ –º–∞—Å–∫–∏. –ü—Ä–∏ —ç—Ç–æ–º –º—ã –æ–±—ä–µ–¥–∏–Ω—è–µ–º feature map (—Å –±—ç–∫–±–æ–Ω–∞) c –º–∞—Å–∫–∞–º–∏ –±–æ–∫—Å–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º —ç—Ç–∞–ø–µ. –†–∞–∑–º–µ—Ä—ã –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ –±—É–¥—É—Ç –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–æ–∫—Å–æ–≤;
- –î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º—ã —Ñ–æ—Ä–º–∏—Ä—É–µ–º [FPN-like](https://arxiv.org/pdf/1612.03144.pdf) –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –≤—ã—à–µ feature map.

> **–í–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç:** **Feature Pyramid Net (FPN),** –∏–ª–∏ **–ø–∏—Ä–∞–º–∏–¥–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤** ‚Äî —Å–≤—ë—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –≤ –≤–∏–¥–µ –ø–∏—Ä–∞–º–∏–¥—ã –∏ —Å–ª—É–∂–∞—â–∞—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤ –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∏–∂–Ω–∏—Ö –∏ –≤–µ—Ä—Ö–Ω–∏—Ö —É—Ä–æ–≤–Ω–µ–π —Å–µ—Ç–∏; –ø–µ—Ä–≤—ã–µ –∏–º–µ—é—Ç –≤—ã—Å–æ–∫–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ, –Ω–æ –Ω–∏–∑–∫—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é, –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å; –≤—Ç–æ—Ä—ã–µ –Ω–∞–æ–±–æ—Ä–æ—Ç.

### –ö–æ–¥ MHAttentionMap, –æ—Ç–≤–µ—á–∞—é—â–µ–≥–æ —Ç–æ–ª—å–∫–æ –∑–∞ –ø–æ–¥—Å—á–µ—Ç attention softmax (–±–µ–∑ —É–º–Ω–æ–∂–µ–Ω–∏—è –Ω–∞ V)
```
import torch
import torch.nn as nn


class MHAttentionMap(nn.Module):
    """
		–ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è MHAttentionMap.
    """
    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0.0, bias=True):
        super().__init__()
        self.num_heads = num_heads
        self.hidden_dim = hidden_dim
        self.dropout = nn.Dropout(dropout)

        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)
        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)
				
				# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤.
        nn.init.zeros_(self.k_linear.bias)
        nn.init.zeros_(self.q_linear.bias)
        nn.init.xavier_uniform_(self.k_linear.weight)
        nn.init.xavier_uniform_(self.q_linear.weight)
        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5

    def forward(self, q, k, mask: Optional[Tensor] = None):
        q = self.q_linear(q)
        k = F.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)
        qh = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)
        kh = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])
        weights = torch.einsum("bqnc,bnchw->bqnhw", qh * self.normalize_fact, kh)

        if mask is not None:
            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float("-inf"))
        weights = F.softmax(weights.flatten(2), dim=-1).view(weights.size())
        weights = self.dropout(weights)
        return weights
```

### –ö–æ–¥ MaskHeadSmallConv ‚Äî FPN-like CNN –≥–æ–ª–æ–≤—ã
```
import torch
import torch.nn as nn


class MaskHeadSmallConv(nn.Module):
		"""
		–ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è MaskHeadSmallConv. Upsampling –¥–µ–ª–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é FPN –ø–æ–¥—Ö–æ–¥–∞.
    """
    def __init__(self, dim, fpn_dims, context_dim):
        super().__init__()
				
				# –í –∫–∞—á–µ—Å—Ç–≤–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º GroupNorm.
				# –ò–¥–µ—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å feature map –≤ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö: 
				# –æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º –º—ã –±—É–¥–µ–º —Å–∫–ª–∞–¥—ã–≤–∞—Ç—å –¥—Ä—É–≥
				# —Å –¥—Ä—É–≥–æ–º.
        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]
        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)
        self.gn1 = torch.nn.GroupNorm(8, dim)
        self.lay2 = torch.nn.Conv2d(dim, inter_dims[1], 3, padding=1)
        self.gn2 = torch.nn.GroupNorm(8, inter_dims[1])
        self.lay3 = torch.nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)
        self.gn3 = torch.nn.GroupNorm(8, inter_dims[2])
        self.lay4 = torch.nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)
        self.gn4 = torch.nn.GroupNorm(8, inter_dims[3])
        self.lay5 = torch.nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)
        self.gn5 = torch.nn.GroupNorm(8, inter_dims[4])
        self.out_lay = torch.nn.Conv2d(inter_dims[4], 1, 3, padding=1)

        self.dim = dim

        self.adapter1 = torch.nn.Conv2d(fpn_dims[0], inter_dims[1], 1)
        self.adapter2 = torch.nn.Conv2d(fpn_dims[1], inter_dims[2], 1)
        self.adapter3 = torch.nn.Conv2d(fpn_dims[2], inter_dims[3], 1)
				
				# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤.
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_uniform_(m.weight, a=1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x: Tensor, bbox_mask: Tensor, fpns: List[Tensor]):
		    """
		    fpns - 3 feature map —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å –±—ç–∫–±–æ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∏–∂–µ
		    –º—ã –±—É–¥–µ–º —Å–∫–ª–µ–∏–≤–∞—Ç—å —Å feature maps, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ —Å–≤—ë—Ä—Ç–∫–∞–º–∏ –≤ mask head.
		    """
		    
		    # –ú—ã –æ–±—ä–µ–¥–∏–Ω—è–µ–º x (feature map —Å backbone) –∏ –º–∞—Å–∫–∞–º–∏ –±–æ–∫—Å–æ–≤,
		    # –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ —ç—Ç–∞–ø–µ MHAttentionMap. –†–∞–∑–º–µ—Ä—ã –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
		    # –±—É–¥—É—Ç –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–æ–∫—Å–æ–≤.
        x = torch.cat([_expand(x, bbox_mask.shape[1]), bbox_mask.flatten(0, 1)], 1)

        x = self.lay1(x)
        x = self.gn1(x)
        x = F.relu(x)
        x = self.lay2(x)
        x = self.gn2(x)
        x = F.relu(x)
				
				# –°—Ç—Ä–æ–∏–º –ø–∏—Ä–∞–º–∏–¥—É –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
        cur_fpn = self.adapter1(fpns[0])
        if cur_fpn.size(0) != x.size(0):
		        # expand - resize feature map cur_fpn –¥–æ —Ä–∞–∑–º–µ—Ä–æ–≤ feature map x.
            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))
        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode="nearest")
        x = self.lay3(x)
        x = self.gn3(x)
        x = F.relu(x)

        cur_fpn = self.adapter2(fpns[1])
        if cur_fpn.size(0) != x.size(0):
            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))
        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode="nearest")
        x = self.lay4(x)
        x = self.gn4(x)
        x = F.relu(x)

        cur_fpn = self.adapter3(fpns[2])
        if cur_fpn.size(0) != x.size(0):
            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))
        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode="nearest")
        x = self.lay5(x)
        x = self.gn5(x)
        x = F.relu(x)

        x = self.out_lay(x)
        return x
```


### –ö–æ–¥ SegmentationDetr
```
import torch
import torch.nn as nn


class DETRsegm(nn.Module):
		"""
		–ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ DETR.
    """
    def __init__(self, detr, freeze_detr=False):
        super().__init__()
        self.detr = detr
				
				# –ö–∞–∫ –±—ã–ª–æ —Å–∫–∞–∑–∞–Ω–æ –≤—ã—à–µ, –º—ã –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å –≤–µ—Å–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞.
				# –≠—Ç–æ –Ω–µ –ø–æ–≤–ª–∏—è–µ—Ç –Ω–∞ –∏—Ç–æ–≥–æ–≤–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —É–ø—Ä–æ—Å—Ç–∏—Ç 
				# –æ–±—É—á–µ–Ω–∏–µ.
        if freeze_detr:
            for p in self.parameters():
                p.requires_grad_(False)

        hidden_dim, nheads = detr.transformer.d_model, detr.transformer.nhead
        self.bbox_attention = MHAttentionMap(hidden_dim, hidden_dim, nheads, dropout=0.0)
        self.mask_head = MaskHeadSmallConv(hidden_dim + nheads, [1024, 512, 256], hidden_dim)

    def forward(self, samples: NestedTensor):
        if isinstance(samples, (list, torch.Tensor)):
            samples = nested_tensor_from_tensor_list(samples)
        features, pos = self.detr.backbone(samples)

        bs = features[-1].tensors.shape[0]

        src, mask = features[-1].decompose()
        assert mask is not None
        src_proj = self.detr.input_proj(src)
        hs, memory = self.detr.transformer(src_proj, mask, self.detr.query_embed.weight, pos[-1])

        outputs_class = self.detr.class_embed(hs)
        outputs_coord = self.detr.bbox_embed(hs).sigmoid()
        out = {"pred_logits": outputs_class[-1], "pred_boxes": outputs_coord[-1]}
        if self.detr.aux_loss:
            out['aux_outputs'] = self.detr._set_aux_loss(outputs_class, outputs_coord)

        # MHSA —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç attention maps –¥–ª—è –±–æ–∫—Å–æ–≤.
        bbox_mask = self.bbox_attention(hs[-1], memory, mask=mask)
				
				# –ê mask head, –∫–æ—Ç–æ—Ä–∞—è –∏ —É—á–∏—Ç –º–∞—Å–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤.
        seg_masks = self.mask_head(src_proj, bbox_mask, [features[2].tensors, features[1].tensors, features[0].tensors])
        outputs_seg_masks = seg_masks.view(bs, self.detr.num_queries, seg_masks.shape[-2], seg_masks.shape[-1])

        out["pred_masks"] = outputs_seg_masks
        return out


def _expand(tensor, length: int):
    return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)
```
## –î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞ DETR

- –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ object queries —á–µ—Ä–µ–∑ self-attention –¥–µ–∫–æ–¥–µ—Ä–∞ –≤–º–µ—Å—Ç–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º matching loss —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–≤–æ–¥—è—Ç –∫¬†**–æ—Ç—Å—É—Ç—Å—Ç–≤–∏—é –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.** –û–¥–Ω–∞–∫–æ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤—Å–µ-—Ç–∞–∫–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è, –ø–æ—ç—Ç–æ–º—É –Ω–∞–∫–∏–Ω—É—Ç—å NMS —Å—Ç–æ–∏—Ç.
- –ö–∞–∫ –∏ –≤ ViT, self-attention –æ—Ç–ª–∏—á–Ω–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–µ–π **—É—á–µ—Ç–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞** –∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—Ç–Ω–æ—à–µ–Ω–∏–π –º–µ–∂–¥—É –¥–∞–ª–µ–∫–∏–º–∏ –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞ —Ç–æ–∫–µ–Ω–∞–º–∏ (–ø–∞—Ç—á–∞–º–∏ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è), –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –æ–±—ã—á–Ω—ã–µ CNN –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã.
- –°–ª–æ–π¬†[MultiHeadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ—Ö–æ–∂–∏–π —Ç—Ä—é–∫ –≤ CV, —á—Ç–æ –∏ –≤ NLP: –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –¥—Ä—É–≥–∏—Ö –∏ –±–µ—Ä–µ—Ç –Ω–∞ —Å–µ–±—è (–∏–ª–∏ –≤–º–µ—Å—Ç–µ —Å –¥—Ä—É–≥–æ–π —á–∞—Å—Ç—å—é –≥–æ–ª–æ–≤) –∫–∞–∫—É—é-–ª–∏–±–æ –ø–æ–¥–∑–∞–¥–∞—á—É. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ NLP —Ç–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ –º–æ–≥—É—Ç –±—Ä–∞—Ç—å –Ω–∞ —Å–µ–±—è –≥–æ–ª–æ–≤—ã:
    - **–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–∞—è**: –∫–∞–∫ —Ç–æ–∫–µ–Ω—ã —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã –¥—Ä—É–≥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥—Ä—É–≥–∞; —á—Ç–æ –∏–¥–µ—Ç –¥–æ / –ø–æ—Å–ª–µ;
    - **—Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è**: –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ (–ø–æ–¥–ª–µ–∂–∞—â–µ–µ-–≥–ª–∞–≥–æ–ª, –≥–ª–∞–≥–æ–ª-–æ–±—ä–µ–∫—Ç);
    - **—á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤**: –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –Ω–∞–∏–º–µ–Ω–µ–µ —á–∞—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.
  –í —Å–ª—É—á–∞–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–æ–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å –ø–æ–¥–∑–∞–¥–∞—á–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:
	- –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –Ω—É–∂–Ω—ã –≥—Ä–∞–Ω–∏—Ü—ã –æ–±—ä–µ–∫—Ç–∞;
	- –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ‚Äî —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç—è—Ö.

## –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ DETR

- **–ü–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –º–∞–ª–µ–Ω—å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–∞—Ö**. DETR –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω scale –∏–∑ –±—ç–∫–±–æ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–µ–±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –ü–æ—á–µ–º—É –ø—Ä–∏ —ç—Ç–æ–º –Ω–µ–ª—å–∑—è –¥–æ–±–∞–≤–∏—Ç—å FPN –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –ø–æ–≤—ã—à–µ / –≤—Å—é –ø–∏—Ä–∞–º–∏–¥—É —Ñ–∏—á–µ–π? –û—Ç–≤–µ—Ç –ø—Ä–æ—Å—Ç–æ–π –∏ –≥—Ä—É—Å—Ç–Ω—ã–π: –æ–ø–µ—Ä–∞—Ü–∏—è self-attention –≤ —ç–Ω–∫–æ–¥–µ—Ä–µ –∏ cross-attention –≤ –¥–µ–∫–æ–¥–µ—Ä–µ –æ—á–µ–Ω—å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã –∫ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ñ–∏—á–µ–π, –ø–æ—Ç–æ–º—É —á—Ç–æ attention –∏–º–µ–µ—Ç –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –Ω–∏—Ö üòû.
- **–ü—Ä–æ–±–ª–µ–º—ã –æ–±—É—á–µ–Ω–∏—è**. –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∞–¥–µ–∫–≤–∞—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ DETR-—É, –∫–∞–∫ –∏ –º–Ω–æ–≥–∏–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º, –Ω—É–∂–Ω–æ –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ –±–æ–ª—å—à–µ —ç–ø–æ—Ö, —á–µ–º –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–º –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞–º. –ù—É –∏ –≤ —Ü–µ–ª–æ–º –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –æ—á–µ–Ω—å —Ç—è–∂–µ–ª–æ –æ–±—É—á–∞—Ç—å –±–æ–ª—å—à–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: –Ω—É–∂–Ω–æ –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω learning rate –∏ scheduling, —á—Ç–æ–±—ã –ª–æ—Å—Å –Ω–µ —É–ª–µ—Ç–µ–ª –≤ NaN –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ.
- **–ü—Ä–æ–±–ª–µ–º—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞**. –ü—Ä–æ–±–ª–µ–º—ã –∑–¥–µ—Å—å –∏–∑-–∑–∞ —Ç–æ–≥–æ, —á—Ç–æ DETR ‚Äî —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –±–æ–ª—å—à–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–∞–º—è—Ç–∏ –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ –≤—Å–ª–µ–¥—Å—Ç–≤–∏–µ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ Attention (–º—ã –¥–æ–ª–∂–Ω—ã –ø–æ—Å—á–∏—Ç–∞—Ç—å —Å–∫–æ—Ä –ø–æ–ø–∞—Ä–Ω–æ –º–µ–∂–¥—É –≤—Å–µ–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏). –ö —Ç–æ–º—É –∂–µ, –Ω–µ –±—É–¥–µ–º –∑–∞–±—ã–≤–∞—Ç—å, —á—Ç–æ Query, Key –∏ Value ‚Äî –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤–Ω–æ—Å—è—Ç —Å–≤–æ–π –≤–∫–ª–∞–¥ –≤ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ latency —á–µ—Ä–µ–∑ MAC.

# CLIP: Contrastive Language-Image Pre-Training  

**Premise** 
CLIP, which stands for Contrastive Language-Image Pretraining, is a deep learning model developed by OpenAI in 2021. CLIP‚Äôs embeddings for images and text share the same space, enabling direct comparisons between the two modalities. This is accomplished by training the model to bring related images and texts closer together while pushing unrelated ones apart.

## CLIP : Idea

CLIP is designed to predict which N √ó N potential (image, text) pairings within the batch are actual matches. To achieve this, CLIP establishes a multi-modal embedding space through the joint training of an image encoder and text encoder. **The CLIP loss aims to maximize the cosine similarity between the image and text embeddings for the N genuine pairs in the batch while minimizing the cosine similarity for the N¬≤ ‚àí N incorrect pairings.** The optimization process involves using a symmetric cross-entropy loss function that operates on these similarity scores. The following presents pseudocode (taken from the original paper) outlining the core implementation of CLIP.

```
# image_encoder - ResNet or Vision Transformer  
# text_encoder - CBOW or Text Transformer  
# I[n, h, w, c] - minibatch of aligned images  
# T[n, l] - minibatch of aligned texts  
# W_i[d_i, d_e] - learned proj of image to embed  
# W_t[d_t, d_e] - learned proj of text to embed  
# t - learned temperature parameter  

# extract feature representations of each modality  
I_f = image_encoder(I) #[n, d_i]  
T_f = text_encoder(T) #[n, d_t]  

# joint multimodal embedding [n, d_e]  
I_e = l2_normalize(np.dot(I_f, W_i), axis=1)  
T_e = l2_normalize(np.dot(T_f, W_t), axis=1)  

# scaled pairwise cosine similarities [n, n]  
logits = np.dot(I_e, T_e.T) * np.exp(t)  

# symmetric loss function  
labels = np.arange(n)  
loss_i = cross_entropy_loss(logits, labels, axis=0)  
loss_t = cross_entropy_loss(logits, labels, axis=1)  
loss = (loss_i + loss_t)/2# image_encoder - ResNet or Vision Transformer  

# text_encoder - CBOW or Text Transformer  
# I[n, h, w, c] - minibatch of aligned images  
# T[n, l] - minibatch of aligned texts  
# W_i[d_i, d_e] - learned proj of image to embed  
# W_t[d_t, d_e] - learned proj of text to embed  
# t - learned temperature parameter  
# extract feature representations of each modality  

I_f = image_encoder(I) #[n, d_i]  
T_f = text_encoder(T) #[n, d_t]  

# joint multimodal embedding [n, d_e]  
I_e = l2_normalize(np.dot(I_f, W_i), axis=1)  
T_e = l2_normalize(np.dot(T_f, W_t), axis=1)  

# scaled pairwise cosine similarities [n, n]  
logits = np.dot(I_e, T_e.T) * np.exp(t)  

# symmetric loss function  
labels = np.arange(n)  
loss_i = cross_entropy_loss(logits, labels, axis=0)  
loss_t = cross_entropy_loss(logits, labels, axis=1)  
loss = (loss_i + loss_t)/2
```

## Model architecture
ClIP uses two separate architectures as the backbone for encoding vision and text datasets:
1. `image_encoder`: Represents the neural network architecture (e.g., ResNet or Vision Transformer) responsible for encoding images.
2. `text_encoder`: Represents the neural network architecture (e.g., CBOW, BERT, or Text Transformer) responsible for encoding textual information.
![[Pasted image 20240417234537.png]]

The model takes a batch of n pairs of images and texts as input where:
- `I[n, h, w, c]`: Represents a minibatch of aligned images, where `n` is the batch size, `h` is the image height, `w` is the image width, and `c` is the number of channels.
- `T[n, l]`: Represents a minibatch of aligned texts, where `n` is the batch size, and `l` is the length of the textual sequence.

### Projections

- `W_i[d_i, d_e]`: Represents the learned projection matrix for mapping image features (`I_f`) to an embedding space (`I_e`). The shape of `W_i` is `[d_i, d_e]`, where `d_e` is the desired dimensionality of the joint embedding space.
- `W_t[d_t, d_e]`: Represents the learned projection matrix for mapping text features (`T_f`) to the same embedding space (`T_e`). The shape of `W_t` is `[d_t, d_e]`.

```
class Projection(nn.Module):  
    def __init__(self, d_in: int, d_out: int, p: float=0.5) -> None:  
        super().__init__()  
        self.linear1 = nn.Linear(d_in, d_out, bias=False)  
        self.linear2 = nn.Linear(d_out, d_out, bias=False)  
        self.layer_norm = nn.LayerNorm(d_out)  
        self.drop = nn.Dropout(p)  
  
    def forward(self, x: torch.Tensor) -> torch.Tensor:  
        embed1 = self.linear1(x)  
        embed2 = self.drop(self.linear2(F.gelu(embed1)))  
        embeds = self.layer_norm(embed1 + embed2)  
        return embeds
```

### Embedding and Normalization
- `I_e = l2_normalize(np.dot(I_f, W_i), axis=1)`: Embeds and normalizes image features in the joint embedding space (`I_e`).
- `T_e = l2_normalize(np.dot(T_f, W_t), axis=1)`: Embeds and normalizes text features in the joint embedding space (`T_e`).

```
class VisionEncoder(nn.Module):  
    def __init__(self, d_out: int) -> None:  
        super().__init__()  
        base = models.resnet34(pretrained=True)  
        d_in = base.fc.in_features  
        base.fc = nn.Identity()  
        self.base = base  
        self.projection = Projection(d_in, d_out)  
        for p in self.base.parameters():  
            p.requires_grad = False  
  
    def forward(self, x):  
        projected_vec = self.projection(self.base(x))  
        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)  
        return projected_vec / projection_len  
  
  
class TextEncoder(nn.Module):  
    def __init__(self, d_out: int) -> None:  
        super().__init__()  
        self.base = AutoModel.from_pretrained(Config.text_model)  
        self.projection = Projection(Config.transformer_embed_dim, d_out)  
        for p in self.base.parameters():  
            p.requires_grad = False  
  
    def forward(self, x):  
        out = self.base(x)[0]  
        out = out[:, 0, :]  # get CLS token output  
        projected_vec = self.projection(out)  
        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)  
        return projected_vec / projection_len  
  
vision_encoder = VisionEncoder(Config.embed_dim)  
I_e = vision_encoder(images)  
caption_encoder = TextEncoder(Config.embed_dim)          
T_e = caption_encoder(text["input_ids"])
```

`logits = np.dot(I_e, T_e.T) * np.exp(t)`: Computes pairwise cosine similarities between image and text embeddings, scaled by a learned temperature parameter `t`.

### Symmetric Loss Function

CLIP uses contrastive loss (first introduced in [Representation Learning with Contrastive Predictive Coding](https://arxiv.org/pdf/1807.03748.pdf)) to bring related images and texts closer together while pushing unrelated ones apart.

```
def CLIP_loss(logits: torch.Tensor) -> torch.Tensor:  
    n = logits.shape[1]      # number of samples  
    labels = torch.arange(n) # Create labels tensor  

    loss_i = F.cross_entropy(logits.transpose(0, 1), labels, reduction="mean")  
    loss_t = F.cross_entropy(logits, labels, reduction="mean")  

    loss = (loss_i + loss_t) / 2  
  
    return loss
```

### Custom simple model
```
class CustomModel(nn.Module):  
    def __init__(self, lr: float = 1e-3) -> None:  
        super().__init__()  
        self.vision_encoder = VisionEncoder(Config.embed_dim)  
        self.caption_encoder = TextEncoder(Config.embed_dim)  
        self.tokenizer = Tokenizer(AutoTokenizer.from_pretrained(Config.text_model))  
        self.lr = lr  
        self.device = "cuda" if torch.cuda.is_available() else "cpu"  
  
    def forward(self, images, text):  
        text = self.tokenizer(text).to(self.device)  
  
        image_embed = self.vision_encoder(images)  
        caption_embed = self.caption_encoder(text["input_ids"])  
        similarity = caption_embed @ image_embed.T  
  
        loss = CLIP_loss(similarity)  
        img_acc, cap_acc = metrics(similarity)  
        return loss, img_acc, cap_acc
```
## Custom dataset training
### Dataset
```
class Flickr30kDataset(torch.utils.data.Dataset):  
    def __init__(self):  
        self.dataset = load_dataset("nlphuji/flickr30k", cache_dir="./huggingface_data")  
        self.transform = transforms.Compose([  
            transforms.Resize((224, 224)),  
            transforms.ToTensor(),  
        ])  
        self.cap_per_image = 2  
  
    def __len__(self):  
        return self.dataset.num_rows["test"] * self.cap_per_image  
  
    def __getitem__(self, idx):  
        original_idx = idx // self.cap_per_image  
        image = self.dataset["test"][original_idx]["image"].convert("RGB")  
        image = self.transform(image)  
  
        # labels  
        caption = self.dataset["test"][original_idx]["caption"][idx % self.cap_per_image]  
  
        return {"image": image, "caption": caption}  
  
flickr30k_custom_dataset = Flickr30kDataset()

from dataclasses import dataclass  
  
  
@dataclass  
class Config:    
    embed_dim: int = 512  # Embedding dimension  
    transformer_embed_dim: int = 768  # Transformer embedding dimension  
    max_len: int = 32  # Maximum text length  
    text_model: str = "distilbert-base-multilingual-cased"  # Text model name  
    epochs: int = 3 # Number of training epochs  
    batch_size: int = 128 # Batch size
```
Model: 
```
# Create an instance of your model  
model = CustomModel().to(device)  
  
# Define optimizer  
optimizer = torch.optim.Adam([  
    {'params': model.vision_encoder.parameters()},  
    {'params': model.caption_encoder.parameters()}  
], lr=model.lr)
```
Training:
```
batch_zero = True  
for epoch in range(start_epoch, num_epochs):  
    model.train()  
    for batch in clip_dataloader:  
        image = batch["image"].to(device)  
        text = batch["caption"]  
        # images, text = batch  
        loss, img_acc, cap_acc = model.common_step((image, text))  
  
        # Backward pass and optimization  
        optimizer.zero_grad()  
        loss.backward()  
        optimizer.step()  
  
        if batch_zero:  
          print(f"Epoch [{0}/{num_epochs}], Batch Loss: {loss.item()}")  
          batch_zero = False  
  
  
    # Print training statistics  
    print(f"Epoch [{epoch+1}/{num_epochs}], Batch Loss: {loss.item()}")  
  
print("Training complete.")
```
## Interacting with API


# DINO: Self-Supervised Vision Transformers
![[Pasted image 20240418002650.png]]**A Student ViT learns to predict global features in an image from local patches supervised by the cross entropy loss from a momentum Teacher ViT‚Äôs embeddings while doing centering and sharpening to prevent mode collapse**

**_Pseudocode_**:
![[Pasted image 20240418003631.png]]

## Premise

The network learns through a process called ‚Äò==self-distillation==‚Äô. There is a teacher and student network both having the same architecture, a **Vision Transformer(ViT)**.

The teacher is a **momentum teacher** which means that it‚Äôs weights are an exponentially weighted average of the student‚Äôs. The momentum teacher was introduced in the paper ‚Äú_Momentum Contrast for Unsupervised Visual Representation Learning‚Äù_ in order to prevent mode collapse when the teacher and the student are the same and output the same embeddings regardless of the input.

The update rule for the teacher‚Äôs weights are:
![[Pasted image 20240418002950.png]]with Œª following a **cosine schedule from 0.996 to 1** during training in this paper.

As is common in self-supervised learning, different crops of one image are taken. Small crops are called Local views( <50% of the image) and large crops( >50% of the image) are called Global views.

All crops are passed through the student while only the global views are passed through the teacher. **_This encourages ‚Äúlocal-to-global‚Äù correspondence, training the student to interpolate context from a small crop._**

Random augmentations of color jittering, Gaussian blur and solarization are also applied on the views to make the network more robust.

## Loss
The teacher and student each predict a 1-dimensional embedding. A softmax along with cross entropy loss is applied to make student‚Äôs distribution match the teacher‚Äôs
**_So we are asking our student network to have the same proportions of features as the teacher._** **_The teacher having a larger context predicts more high level features which the student must also match._**

The cross-entropy loss tries to make the two distributions the same just as in knowledge distillation.

This can also be seen as a made up classification problem. **_We are asking our network to make up a classification problem such that the network can learn meaningful global representations from local views._**

## Centering and sharpening

There are two forms of mode collapse: regardless of the input, the model output is always the same along all the dimensions(i.e same output for any input) or dominated by one dimension. Centering and Sharpening aim to prevent both these.

**Centering**: The teacher‚Äôs raw activations have the their exponentially moving average subtracted from them. It simply is: 
```
Logits = Logits - Logits_mean
```
**_This means that activations must be sometimes positive when they are above their mean and sometimes negative when they are below._** This prevents any one feature from dominating as the mean will be somewhere in the middle of the range. And we know that softmax gives very low values to negative numbers and high values to positive ones.

**Sharpening_: Sharpening is the same as applying a temperature to the softmax to artificially make the distribution more peaked_**, i.e exaggerate small differences so that there is one or some high values and some low values. This prevents all the activations from being the same value as small differences are exaggerated. This acts in synergy with centering which keeps changing which activations are high. Sharpening also helps the student get a stronger signal which features it should increase.

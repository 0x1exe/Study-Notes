
## Bootstrapping
The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.

### Intro
**The process for building one sample can be summarized as follows**:
1. Choose the size of the sample.
2. While the size of the sample is less than the chosen size
    1. Randomly select an observation from the dataset
    2. Add it to the sample

**The bootstrap method can be used to estimate a quantity of a population**. This is done by repeatedly taking small samples, calculating the statistic, and taking the average of the calculated statistics. We can summarize this procedure as follows:

1. Choose a number of bootstrap samples to perform
2. Choose a sample size
3. For each bootstrap sample
    1. Draw a sample with replacement with the chosen size
    2. Calculate the statistic on the sample
4. Calculate the mean of the calculated sample statistics.

**The procedure can also be used to estimate the skill of a machine learning model.**
1. Choose a number of bootstrap samples to perform
2. Choose a sample size
3. For each bootstrap sample
    1. Draw a sample with replacement with the chosen size
    2. Fit a model on the data sample
    3. Estimate the skill of the model on the out-of-bag sample.
4. Calculate the mean of the sample of model skill estimates.

**Importantly, any data preparation prior to fitting the model or tuning of the hyperparameter of the model must occur within the for-loop on the data sample. This is to avoid data leakage where knowledge of the test dataset is used to improve the model. This, in turn, can result in an optimistic estimate of the model skill.**

A useful feature of the bootstrap method is that the resulting sample of estimations often forms a Gaussian distribution. In additional to summarizing this distribution with a central tendency, measures of variance can be given, such as standard deviation and standard error. Further, a confidence interval can be calculated and used to bound the presented estimate. This is useful when presenting the estimated skill of a machine learning model.


The¬†[resample() scikit-learn function](http://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html)¬†can be used. It takes as arguments the data array, whether or not to sample with replacement, the size of the sample, and the seed for the pseudorandom number generator used prior to the sampling.

For example, we can create a bootstrap that creates a sample with replacement with 4 observations and uses a value of 1 for the pseudorandom number generator.
```python
boot = resample(data, replace=True, n_samples=4, random_state=1)
```
### Application
```
# train model
reg = LogisticRegression(random_state=0)
reg.fit(x_train, y_train)

# bootstrap predictions
accuracy = []
n_iterations = 1000
for i in range(n_iterations):
    X_bs, y_bs = resample(x_train, y_train, replace=True)
    # make predictions
    y_hat = reg.predict(X_bs)
    # evaluate model
    score = accuracy_score(y_bs, y_hat)
    accuracy.append(score)
```
Let‚Äôs plot a distribution of accuracy values computed on the bootstrap samples.

```
import seaborn as sns
# plot distribution of accuracy
sns.kdeplot(accuracy)
plt.title("Accuracy across 1000 bootstrap samples of the held-out test set")
plt.xlabel("Accuracy")
plt.show()
```
We can now take the mean accuracy across the bootstrap samples, and compute confidence intervals. There are several different approaches to computing the confidence interval. We will use the percentile method, a simpler approach that does not require our sampling distribution to be normally distributed.
```
# get median
median = np.percentile(accuracy, 50)

# get 95% interval
alpha = 100-95
lower_ci = np.percentile(accuracy, alpha/2)
upper_ci = np.percentile(accuracy, 100-alpha/2)

print(f"Model accuracy is reported on the test set. 1000 bootstrapped samples " 
      f"were used to calculate 95% confidence intervals.\n"
      f"Median accuracy is {median:.2f} with a 95% a confidence "
      f"interval of [{lower_ci:.2f},{upper_ci:.2f}].")
```
**Output**
```
Model accuracy is reported on the test set. 1000 bootstrapped samples were used to calculate 95% confidence intervals.
Median accuracy is 0.86 with a 95% a confidence interval of [0.80,0.91].
```
The confidence interval tells us about the reliability of the estimation procedure. 95% of confidence intervals computed at the 95% confidence level contain the true value of the parameter.

### ‚ö†Ô∏èKey points
- Bootstrapping is a resampling technique, sometimes confused with cross-validation.
- Bootstrapping allows us to generate a distribution of estimates, rather than a single point estimate.
- Bootstrapping allows us to estimate uncertainty, allowing computation of confidence intervals.

## Bagging
Bagging, which is short for¬†_bootstrap aggregating_, builds off of bootstrapping. Bootstrap aggregating describes the process by which multiple models of the same learning algorithm are trained with bootstrapped samples of the original data set.

**Ensemble machine learning can be mainly categorized into bagging and boosting.**

## ‚è± –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
#### –†–∞–±–æ—Ç–∞ —Å¬†`pickle`
`Pickle`¬†- —ç—Ç–æ –æ—Ç–ª–∏—á–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –ø—Ä–∏–≤—ã—á–Ω—ã–º –Ω–∞–º¬†`.csv`¬†—Ñ–∞–π–ª–∞–º –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –±–æ–ª—å—à–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏. –ú–∞–ª–æ —Ç–æ–≥–æ, —á—Ç–æ –æ–Ω —Å—á–∏—Ç—ã–≤–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å—ë –≤ —Ä–∞–∑—ã –±—ã—Å—Ç—Ä–µ–µ, —Ç–∞–∫ –µ—â–µ –∏ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ —Ç–∞–∫–æ–π —Ñ–∞–π–ª –∑–∞–Ω–∏–º–∞–µ—Ç –º–µ–Ω—å—à–µ. –¢–∞–∫–∂–µ, –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏¬†`to_pickle()`, —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∏–Ω–¥–µ–∫—Å—ã –∏ –≤—Å–µ —Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫, —Ç–∞–∫ —á—Ç–æ –ø—Ä–∏ –µ–≥–æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º —Å—á–∏—Ç—ã–≤–∞–Ω–∏–∏, –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –±—É–¥–µ—Ç —Ç–æ—á–Ω–æ —Ç–∞–∫–∏–º –∂–µ –∏ –µ–≥–æ –Ω–µ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –∫–∞–∂–¥–æ–º –æ—Ç–∫—Ä—ã—Ç–∏–∏, –∫–∞–∫ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏¬†`CSV`¬†—Ñ–æ—Ä–º–∞—Ç–∞

```
pd.to_pickle()
pd.read_pickle()
```
#### –°—á–∏—Ç—ã–≤–∞–Ω–∏–µ –ø–æ –±–∞—Ç—á–∞–º
```
import gc

chunksize = 1000
tmp_lst = []
with pd.read_csv('../data/car_train.csv',
                 index_col='car_id',
                 dtype={'model': 'category',
                        'car_type': 'category',
                        'fuel_type': 'category',
                        'target_class': 'category'}, chunksize=chunksize) as reader:
    for chunk in reader:
        tmp_lst.append(chunk)
        
data = pd.concat(tmp_lst)

del tmp_lst
gc.collect()

data.head()
```

**–í–∞–∂–Ω–æ:**¬†–ü–æ–∫–∞ –Ω–∞ –∫–∞–∫–æ–π-—Ç–æ –æ–±—ä–µ–∫—Ç –≤ –ø–∞–º—è—Ç–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–Ω–æ –Ω–µ —É–¥–∞–ª–∏–ª–∏ –∏–ª–∏ –Ω–µ –ø–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–∏–ª–∏ - —ç—Ç–æ—Ç –æ–±—ä–µ–∫—Ç –±—É–¥–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å, —Ö–æ—Ç—è –æ–Ω –º–æ–∂–µ—Ç –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è. –ü–æ—ç—Ç–æ–º—É –≤—Å–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –±–æ–ª—å—à–µ –Ω–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è, –ª—É—á—à–µ —è–≤–Ω–æ —É–¥–∞–ª—è—Ç—å, –∏—Å–ø–æ–ª—å–∑—É—è¬†`del`¬†–∏ –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞—Ç—å —Å–±–æ—Ä—â–∏–∫ –º—É—Å–æ—Ä–∞¬†`gc`¬†-¬†`garbage collector`, –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ –≤—ã—à–µ.

#### Using a generator
```
def read_file(filename):
    with open(filename, 'r') as f:
        for line in f:
            yield line.strip()
```
```
it = read_file('../data/car_info.csv')
next(it)
```




### üóú –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
#### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö —Ç–∏–ø–æ–≤
```
def reduce_mem_usage(df):
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))

    for col in df.columns:
        col_type = df[col].dtype.name

        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))

    return df
```
And then: 
```
df = reduce_mem_usage(df)
df.info()
```
#### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ñ–∏—á–µ–π
```
def convert_columns_to_catg(df, column_list):
    for col in column_list:
        print("converting", col.ljust(30), "size: ", round(df[col].memory_usage(deep=True)*1e-6,2), end="\t")
        df[col] = df[col].astype("category")
        print("->\t", round(df[col].memory_usage(deep=True)*1e-6,2))
        
```
###  ü•å –£—Å–∫–æ—Ä–µ–Ω–∏–µ –ø—Ä–∏ –ø–æ–º–æ—â–∏¬†`Numpy`
```
a = list(range(1_000_000))
b = np.arange(1_000_000)
```
–ü—Ä–∏–º–µ—Ä—ã:
```
10000 in b < 10000 in a

b + 10 < [el+10 for el in a]

b.max() < max(b)
```
–ï—Å–ª–∏ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –¥—Ä—É–≥–æ–º –º–∞—Å—Å–∏–≤–µ –∏ —Ç–æ—Ç "–¥—Ä—É–≥–æ–π" –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π, —Ç–æ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å¬†`set()`, —Ç–∞–∫ –∫–∞–∫ –≤ –Ω–µ–º –ø–æ–∏—Å–∫ —ç–ª–µ–º–µ–Ω—Ç–∞ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è –∑–∞¬†`O(log(n))`, –∞ –≤¬†`np.isin()`¬†–∑–∞¬†`O(n)`. –¢–∞–∫ —á—Ç–æ, –¥–∞–∂–µ –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ¬†`numpy`¬†–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω,¬†`set()`¬†–≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç –µ–≥–æ –ø–æ –≤—Ä–µ–º–µ–Ω–∏.
```
st=set(b)
[el in st for el in a] < np.isin(a,b)

np.where() < if-function

np.vectorize() > np.where()

np.select(conditions, choices, default=3) -> like np.where weith multiple conditions

np.where(df_cars['car_rating'] > 5, df_cars['car_type'].map(mydict), np.nan) < apply mapping function to df
```
**–£—Å–∫–æ—Ä—è–µ–º groupby**
1. –°—á–∏—Ç–∞–µ–º –∫–æ–ª-–≤–æ –∑–Ω–∞—á–µ–Ω–∏–π –≤ –≥—Ä—É–ø–øe
```
df_cars.groupby(['model', 'fuel_type'])['target_reg'].count()



df_cars['int_model'] = lbl.fit_transform((df_cars['model'] + df_cars['fuel_type']).astype(str))
np.bincount(df_cars['int_model'])
```
2. –°—É–º–º–∞/—Å—Ä–µ–¥–Ω–µ–µ
```
df_cars.groupby(['model', 'fuel_type'])['target_reg'].sum()

np.bincount(df_cars['int_model'], weights=df_cars['target_reg'])
```
3. Min/max
```
df_cars.groupby(['int_model'])['target_reg'].agg(['max'])

indices = df_cars['int_model']
max_values = np.maximum.reduceat(df_cars['target_reg'].values[np.argsort(indices)],
                                 np.concatenate(([0], np.cumsum(np.bincount(indices))))[:-1])
```

### ‚ö°Ô∏è¬†`Numba Jit`
Simple example: 
```
from numba import jit

@jit(nopython=True)
def f(n):
    s = 0.
    for i in range(n):
        s += i ** 0.5
    return s
```
Example with pandas dataframe:
```
@jit(nopython=True)
def monotonically_increasing(a):
    max_value = 0
    for i in range(len(a)):
        if a[i] > max_value:
            max_value = a[i]
        a[i] = max_value
    return a
    
%%timeit
monotonically_increasing(df_cars['target_reg'].values)
```
### üßµ Multiprocessing
–° –ø–æ–º–æ—â—å—é¬†`multiprocessing`, –º–æ–∂–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å –≤–æ–æ–±—â–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤—Å–µ. –û–Ω –ø–æ–∑–≤–ª–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ –æ–¥–Ω–æ —è–¥—Ä–æ –≤–∞—à–µ–≥–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞, –∞ —Å—Ä–∞–∑—É –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, —É—Å–∫–æ—Ä–∏—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (—É–∂–µ –Ω–µ —Ç–æ–ª—å–∫–æ¬†`io-bound`, –Ω–æ –µ—â–µ –∏¬†`cpu-bound`) –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑, –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —è–¥–µ—Ä.

```
def parallelize_dataframe(df, func, n_cores=4):
    df_split = np.array_split(df, n_cores)
    pool = Pool(n_cores)
    df = np.concatenate(pool.map(func, df_split))
    pool.close()
    pool.join()
    return df
%%time
tmp = parallelize_dataframe(df, text_prepare.many_row_prepare)
```

##  üö£ –ü–∞—Ä—Å–∏–Ω–≥ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö
### –ê–Ω–∞–ª–∏–∑ URL –∑–∞–ø—Ä–æ—Å–∞
–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –¥–ª—è –Ω–∞—á–∞–ª–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É URL –∞–¥—Ä–µ—Å–∞, —ç—Ç–æ –≤–∞–∂–Ω–æ! URL –∞–¥—Ä–µ—Å –∏–º–µ–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç:

- –º–µ—Ç–æ–¥ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ä–µ—Å—É—Ä—Å—É, –∫–æ—Ç–æ—Ä—ã–π —Ç–∞–∫–∂–µ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è —Å–µ—Ç–µ–≤—ã–º¬†**–ø—Ä–æ—Ç–æ–∫–æ–ª–æ–º**;
- **–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –¥–æ—Å—Ç—É–ø–∞**;
- **—Ö–æ—Å—Ç—ã**¬†‚Äì DNS –∞–¥—Ä–µ—Å, –∫–æ—Ç–æ—Ä—ã–π —É–∫–∞–∑–∞–Ω –∫–∞–∫ IP –∞–¥—Ä–µ—Å;
- **–ø–æ—Ä—Ç**¬†‚Äì –µ—â–µ –æ–¥–Ω–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –¥–µ—Ç–∞–ª—å, –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–µ —Å IP –∞–¥—Ä–µ—Å–æ–º;
- **—Ç—Ä–µ–∫**¬†‚Äì –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Ç–æ–¥–µ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–∞;
- **–ø–∞—Ä–∞–º–µ—Ç—Ä**¬†‚Äì –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å–∞ –æ —Ñ–∞–π–ª–µ.

```
from urllib.parse import urlparse, parse_qsl, parse_qs
url = "http://www.example.com:80/path/to/myfile.html?key1=value1&key2=value2#SomewhereInTheDocument"
url_parsed = urlparse(url)
url_parsed
```
Parse result: `ParseResult(scheme='http', netloc='[www.example.com:80](http://www.example.com/)', path='/path/to/myfile.html', params='', query='key1=value1&key2=value2', fragment='SomewhereInTheDocument')`

The example of POST request:
```
page = requests.post('https://controlc.com/index.php?act=submit', data={
¬† ¬† 'subdomain': '',
¬† ¬† 'antispam': 1,
¬† ¬† 'website': '',
¬† ¬† 'paste_title': '–ó–∞–º–µ—Ç–∫–∞',
¬† ¬† 'input_text': '–ü—Ä–∏–≤–µ—Ç!',
¬† ¬† 'timestamp': 'ba68753935524ba7096650590c86633b',
¬† ¬† 'paste_password': '',
¬† ¬† 'code': 0,
}, headers={'accept-encoding': 'identity', 'referer': 'https://controlc.com/'})
page
```
### Parsing of HTML page with BeautifulSoup
```
from bs4 import BeautifulSoup
import pandas as pd
from tqdm import tqdm
from operator import attrgetter, itemgetter

soup = BeautifulSoup(open('../data/parsing_data/689066_2.html', 'rb').read(), 'lxml')
```
–ë–µ—Ä–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ —Å—Ç—Ä–∞–Ω–∏—á–∫–∏. –ó–∞–ø—É—Å—Ç–∏–≤ —Å–µ—Å—Å–∏—é¬†`BeautifulSoup`, –Ω–∞–π–¥–µ–º —Ç–µ–≥–∏, –∑–¥–µ—Å—å —ç—Ç–æ –º–æ–∂–Ω–æ –≥–æ—Ä–∞–∑–¥–æ –ø—Ä–æ—â–µ –∏ –ø—Ä–∏—è—Ç–Ω–µ–µ –¥–ª—è –≥–ª–∞–∑ -¬†`.find("tag", attribute="value")`. –î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –≤—Å–µ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º¬†`.find_all("tag")`
```
desc = soup.find('div', itemprop='description')
desc = soup.find('div', class_=lambda s: s and s.startswith("styles_synopsisSection")).find_all('p')

film_info = {
¬† ¬† 'title': soup.find('h1', itemprop='name').find('span').text,
¬† ¬† 'title-original': soup.find('span', class_=lambda s: s and s.startswith('styles_originalTitle__')).text,
¬† ¬† 'rating': float(soup.find('a', {'class': 'film-rating-value'}).text),
¬† ¬† 'description': '\n'.join(map(attrgetter('text'), desc))
}
film_info
```
### Selenium



## üêçüî• –î–≤—É—Ö–≥–æ–ª–æ–≤–∞—è –Ω–µ–π—Ä–æ–Ω–∫–∞ –Ω–∞¬†`PyTorch`.
### Self-implemented

```
from torch.utils.data import Dataset, DataLoader
import torch
from torch import nn
import copy

train, test = train_test_split(df, test_size=0.2, random_state=42)

device = torch.device('cpu')

# –í–ê–ñ–ù–û! - —Ñ–∏–∫—Å–∏—Ä—É–µ–º –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å
def seed_everything(seed=42):
    
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    
seed_everything(seed=42)
```
Make a configuration class:
```
class CFG:
    hidden_size=128
    dropout=0.1
    lr=1e-3
    batch_size=128
    num_workers=4
    epochs=20
    num_features=train.shape[1]-2
    num_tar_class=train.target_class.nunique() 
```
A dataset for our table:
```
class Rides(Dataset):
    def __init__(self, df):
        self.df = df
    def __len__(self):
        return len(self.df)
    def __getitem__(self, idx):
        row = self.df.iloc[idx,:]
        data = row.drop(labels=['target_reg', 'target_class'])
        data = torch.FloatTensor(data.values.astype('float'))
        tar_reg = torch.tensor(row['target_reg']).float()
        tar_class = row['target_class'].astype('int')
        
        return data, tar_reg, tar_class
train_datasets = {'train': Rides(train),
                  'val': Rides(test)}
```
Dataloaders:
```
dataloaders_dict = {x: torch.utils.data.DataLoader(train_datasets[x], batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)
	for x in ['train', 'val']}
```
Now the network:
```
class TabularNN(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.mlp = nn.Sequential(
                          nn.Linear(cfg.num_features, cfg.hidden_size),
                          #nn.BatchNorm1d(cfg.hidden_size),
                          nn.Dropout(cfg.dropout),
                          nn.ReLU(),
                          nn.Linear(cfg.hidden_size, cfg.hidden_size),
                          #nn.BatchNorm1d(cfg.hidden_size),
                          nn.Dropout(cfg.dropout),
                          nn.ReLU(),
                          nn.Linear(cfg.hidden_size, cfg.hidden_size//2),
                          )
        
        self.regressor = nn.Sequential(
            nn.Linear(cfg.hidden_size // 2, 1)
        )
        self.classifier = nn.Sequential(
            nn.Linear(cfg.hidden_size // 2, cfg.num_tar_class)
        )

    def forward(self, data):
        x = self.mlp(data)
        tar_reg = self.regressor(x)
        tar_class = self.classifier(x)
        return tar_reg.view(-1), tar_class
```
Initialize it and optimizer/critetion:
```
model = TabularNN(CFG).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr = CFG.lr)
regression_criterion = nn.MSELoss().to(device)
classification_criterion = nn.CrossEntropyLoss().to(device)
```
Training code:
```
def train_model(model, dataloaders, regression_criterion,
                classification_criterion, optimizer, num_epochs=25,
                early_stopping_rounds=5, verbose=2):

    val_acc_history = []

    best_model_wts = copy.deepcopy(model.state_dict())
    best_loss = np.inf
    early_steps = 0
    stop = False

    for epoch in range(num_epochs):
        if stop:
            break
        if epoch % verbose == 0:
            print('Epoch {}/{}'.format(epoch, num_epochs - 1))
            print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0

            # Iterate over data.
            for inputs, labels_1, labels_2 in dataloaders[phase]:
                inputs = inputs.to(device)
                labels_1 = labels_1.to(device)
                labels_2 = labels_2.to(device)
                
                # zero the parameter gradients
                optimizer.zero_grad()
                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    # Get model outputs and calculate loss

                    outputs_1, outputs_2 = model(inputs)
                    loss_1 = regression_criterion(outputs_1, labels_1)
                    loss_2 = classification_criterion(outputs_2, labels_2)

                    loss = loss_1 + loss_2

                    _, preds_2 = torch.max(outputs_2, 1)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                        #scheduler.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                val_acc_history.append(running_loss)

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            if epoch % verbose == 0:
                print('{} Loss: {:.4f}'.format(phase, epoch_loss))

            # deep copy the model
            if phase == 'val' and epoch_loss < best_loss:
                best_model_wts = copy.deepcopy(model.state_dict())
                best_loss = epoch_loss
                early_steps = 0
            if phase == 'val' and epoch_loss > best_loss:
                early_steps += 1
                if early_steps > early_stopping_rounds:
                    stop = True
                    print(f'Stopped by early_stopping. Epoch: {epoch}')
                    break 
    # load best model weights
    model.load_state_dict(best_model_wts)
    return model
    
model_ft = train_model(model, dataloaders_dict, regression_criterion,
                classification_criterion, optimizer, num_epochs=22)
```
Onto predictions:
```
# p1, p2 - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è; l1, l2  - –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

p1, p2, l1, l2 = [], [], [], []

with torch.set_grad_enabled(False):
    # Get model outputs and calculate loss
    for inputs, labels_1, labels_2 in dataloaders_dict['val']:
        inputs = inputs.to(device)
        labels_1 = labels_1.to(device)
        labels_2 = labels_2.to(device)
        l1.extend(labels_1.detach().cpu().numpy())
        l2.extend(labels_2.detach().cpu().numpy())
        
        outputs_1, outputs_2 = model_ft(inputs)
        _, outputs_2 = torch.max(outputs_2, 1)

        p1.extend(outputs_1.detach().cpu().numpy())
        p2.extend(outputs_2.detach().cpu().numpy())
        
torch.save(model_ft.state_dict(), 'tab_model.pth')
```
##   üï∏¬†`TabNet`¬†- "SOTA" –¥–ª—è —Ç–∞–±–ª–∏—á–µ–∫.
```
y = df['target_class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from pytorch_tabnet.tab_model import TabNetClassifier

clf = TabNetClassifier(device_name='cpu')
clf.fit(X_train.values,
        y_train,
        patience=100,
        eval_set=[(X_test.values, y_test)])
```
We can also view feature importance:
```
for i, j in sorted(zip(clf.feature_importances_.astype('float16'), X_train.columns), reverse=True):
    print(i, j)
```
[–ü—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ TabNet](https://www.kaggle.com/code/carlmcbrideellis/tabnet-and-interpretability-jane-street-example)
```
import matplotlib.pyplot as plt

explain_matrix, masks = clf.explain(X_test.values)

fig, axs = plt.subplots(1, 3, figsize=(20,20))
for i in range(3):
    axs[i].imshow(masks[i][:50])
    axs[i].set_title(f"mask {i}")
    
```

```
normalized_explain_mat = np.divide(explain_matrix, explain_matrix.sum(axis=1).reshape(-1, 1)+1e-8)

# Add prediction to better understand correlation between features and predictions
preds = clf.predict(X_test.values)

explain_and_preds = np.hstack([normalized_explain_mat, preds.reshape(-1, 1)])

correlation_importance = np.corrcoef(explain_and_preds.T)
px.imshow(correlation_importance,
          labels=dict(x="Features", y="Features", color="Correlation"),
          x=list(X_test.columns)+["prediction"], y=list(X_test.columns)+["prediction"],
          title="Correlation between attention mechanism for each feature and predictions",
          width=1000,
          height=1000,
          color_continuous_scale='Jet')
```


## üöö Extracting embeddings
### TabularNetworks
Just return the result from last layer without classification/regression head
```
# –ó–∞–≥—Ä—É–∂–∞–µ–º, —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ä–∞–Ω–µ–µ, –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
model = TabularNN(CFG)
PATH = 'tab_model.pth'
model.load_state_dict(torch.load(PATH))

# –û–±—ã—á–Ω–æ, –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏ –∑–∞–º–µ–Ω—è—é—Ç –Ω–∞ nn.Identity()
# –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ, —ç—Ç–æ –¥–µ–ª–∞—Ç—å –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ
model.classifier = torch.nn.Identity()
model.regressor = torch.nn.Identity()
model.to(device)
model.eval()
```
Now extract embeddings:
```
embeddings = []

with torch.no_grad():
    for inputs, labels_1, labels_2 in dataloader:
        inputs = inputs.to(device)
        outputs = model(inputs)
        embeddings.extend(outputs.detach().cpu().numpy())
embed_df = pd.DataFrame(data=embeddings,columns=[f'embed_{i}' for i in range(embeddings[0].shape[0])])

df = pd.concat((df, embed_df) ,axis=1)
df.head()
```
Train CatBoost on that data
### LLMs

####  üßµ TextEmbeddings
```
class TextEmbeddings:
    def __init__(self, add_cls_embeddings=True, add_mean_embeddings=False):
        self.add_mean_embeddings = add_mean_embeddings
        self.add_cls_embeddings = add_cls_embeddings
        if add_cls_embeddings is False and add_mean_embeddings is False:
            raise 'Error: you should select at least one type of embeddings to be computed'

    def mean_pooling(self, hidden_state, attention_mask):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π —Å —É—á–µ—Ç–æ–º attention_mask hidden_state.
        """
        token_embeddings = hidden_state.detach().cpu() 
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
        return sum_embeddings / attention_mask.sum()

    def extract_embeddings(self, texts, model_name, max_len):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è, –ø–æ—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é - —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ texts.
        """
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name).cuda()
        text_features = []
        for sentence in tqdm(texts):
            encoded_input = tokenizer([sentence],
                                      padding='max_length',
                                      truncation=True,
                                      max_length=max_len,
                                      return_tensors='pt')
            with torch.no_grad():
                hidden_state, cls_head = model(input_ids=encoded_input['input_ids'].cuda(), return_dict=False)
                sentence_embeddings = self.mean_pooling(hidden_state, encoded_input['attention_mask'])
            
            now_emb = []
            if self.add_cls_embeddings:
                now_emb.append(cls_head.detach().cpu().numpy().flatten())
            
            if self.add_mean_embeddings:
	            now_emb.append(sentence_embeddings.detach().cpu().numpy().flatten())
            
            text_features.append(np.concatenate(now_emb, axis=0))
        return text_features

    def add_many_embeddings(self, df, text_col, models):
        """"
        –î–æ–±–∞–≤–ª—è–µ—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ text_col.
        –í –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–µ–π –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –¥–ª–∏–Ω –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è models.
        """
        for model_name, max_len in models:
            print(model_name)
            text_features = self.extract_embeddings(df[text_col], model_name, max_len)
            text_features_df = pd.DataFrame(text_features, columns = [f'{model_name}_{text_col}_feature_{i}' for i in range(len(text_features[0]))])
            df = df.join(text_features_df)
            df.to_csv('transformers_text_features.csv', index=False)
            os.system('cp /content/transformers_text_features.csv /content/drive/MyDrive/datasets/transformers_text_features.csv')
        return df
```

#### üó£ –°–æ–≤–µ—Ç—ã –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π

–ù–∞–ø–æ—Å–ª–µ–¥–æ–∫, –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–≤–µ—Ç–æ–≤ –ø–æ —Ä–∞–±–æ—Ç–µ —Å —Ç–µ–∫—Å—Ç–∞–º–∏.

**–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏:**

1. –¢–µ–∫—Å—Ç—ã –Ω–µ –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–µ (–≤ –∏–¥–µ–∞–ª–µ –¥–æ 512 —Ç–æ–∫–µ–Ω–æ–≤)
2. –í—ã —Ö–æ—Ç–∏—Ç–µ –æ–ø–∏—Ä–∞—Ç—å—Å—è –Ω–∞ —Å–º—ã—Å–ª —Ç–µ–∫—Å—Ç–∞, –∞ –Ω–µ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
3. –ï—Å—Ç—å GPU —Ä–µ—Å—É—Ä—Å—ã –∏ –æ—Ç –≤–∞—Å –Ω–µ —Ç—Ä–µ–±—É—é—Ç –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã

**–ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**

1. –õ—É—á—à–µ –≤—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –≤–∞—à–µ–º –¥–æ–º–µ–Ω–µ, –µ—Å–ª–∏ —Ç–∞–∫–∏–µ –µ—Å—Ç—å
2. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤–∞—à–∞ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º –≤–∞–º —è–∑—ã–∫–æ–º
3. –ï—Å–ª–∏ –æ—á–µ–Ω—å —Ö–æ—á–µ—Ç—Å—è, —Ç–æ —Ç–µ–∫—Å—Ç—ã –º–æ–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π (—Å–º. —É—Ä–æ–∫ –ø—Ä–æ –ø–∞—Ä—Å–∏–Ω–≥) –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ —É–∂–µ –¥–ª—è –ø–µ—Ä–µ–≤–µ–¥–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
4. –ú–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –¥–æ–æ–±—É—á–∞—Ç—å –Ω–∞ –≤–∞—à–µ–π –∑–∞–¥–∞—á–µ
5. –ú–æ–∂–Ω–æ –¥–æ—Å—Ç–∞–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∞ –º–æ–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ç–∞—Ä–≥–µ—Ç —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏
6. –ï—Å–ª–∏ –µ—Å—Ç—å –≤—Ä–µ–º—è –∏ –º–æ—â–Ω–æ—Å—Ç–∏, —Ç–æ –Ω–µ –ø—Ä–µ–Ω–µ–±—Ä–µ–≥–∞–π—Ç–µ —Å—Ç–µ–∫–∏–Ω–≥–æ–º —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π


## üèãÔ∏è‚Äç‚ôÇÔ∏èüèåÔ∏è‚Äç‚ôÇÔ∏è Weigths & Biases
#### üîë –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- –ü–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Å—Ä–∞–∑—É –∂–µ –≤—ã—è–≤–ª—è—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –º–µ—Å—Ç–∞.
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ - –≥—Ä–∞—Ñ–∏–∫–∏, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∏–¥–µ–æ, –∞—É–¥–∏–æ, 3D-–æ–±—ä–µ–∫—Ç—ã –∏ –º–Ω–æ–≥–æ–µ¬†[–¥—Ä—É–≥–æ–µ](https://docs.wandb.ai/guides/track/log#logging-objects).
- –ü–æ–∑–≤–æ–ª—è–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –∏ —Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ, —Ç–∞–∫–∂–µ —É–¥–æ–±–Ω–æ –ø—Ä–∏ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Ä–∞–±–æ—Ç–µ –Ω–∞–¥ –∑–∞–¥–∞—á–µ–π
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤¬†`W&B`¬†–ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–µ—Ä—Å–∏–∏ –≤–∞—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –º–æ–¥–µ–ª–µ–π, –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
- –ù–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –≤—Ö–æ–¥–∞ (–º–æ–∂–Ω–æ –Ω–∞—á–∞—Ç—å —Å 6 —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞), —Ç–∞–∫–∂–µ –µ—Å—Ç—å –≥–æ—Ç–æ–≤—ã–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–æ –≤—Å–µ–º–∏ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ DS —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º–∏

#### üì≤ –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —É—á–µ—Ç–Ω–æ–π –∑–∞–ø–∏—Å–∏ –∏ –≤—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É
–ù–∞¬†`Kaggle`¬†–º–æ–∂–Ω–æ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å—Å—è –≤¬†`W&B`¬†–¥–≤—É–º—è —Å–ø–æ—Å–æ–±–∞–º–∏:

1. –° –ø–æ–º–æ—â—å—é¬†`wandb.login ()`. –û–Ω –∑–∞–ø—Ä–æ—Å–∏—Ç –∫–ª—é—á API, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –º–æ–∂–µ—Ç–µ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å + –≤—Å—Ç–∞–≤–∏—Ç—å.
2. –ò—Å–ø–æ–ª—å–∑—É—è¬†`Kaggle secrets`¬†–¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–ª—é—á–∞ API –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–π –Ω–∏–∂–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∫–æ–¥–∞ –¥–ª—è –≤—Ö–æ–¥–∞ –≤ —Å–∏—Å—Ç–µ–º—É. 
```
from kaggle_secrets import UserSecretsClient

user_secrets = UserSecretsClient()
wandb_api = user_secrets.get_secret("wandb_api") 
wandb.login(key=wandb_api)
```
####  ‚úçÔ∏è –ó–∞–ª–æ–≥–∏—Ä—É–µ–º, –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞, —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é¬†`MLP`
```
import torch
import os
import copy
import sys
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from torch import nn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
```
Save hyperparameters for later use: 
```
CONFIG = dict (
    hidden_size=128,
    dropout=0.1,
    lr=1e-3,
    batch_size=128,
    num_workers=os.cpu_count(),
    epochs=20,
    num_features=train.shape[1]-2, # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π, –ø–æ–¥–∞–≤–∞–µ–º–æ–µ –Ω–∞ –≤—Ö–æ–¥
    num_tar_2=train.target_class.nunique(), # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–æ–≤ —Ä–∞–≤–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º—ã—Ö –∫–ª–∞—Å—Å–æ–≤
    architecture = "MLP",
    infra = "Kaggle"
)
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
model = TabularNN(CONFIG).to(device)

# –æ–ø—Ç–∏–º–∞–π–∑–µ—Ä –∏ –ª–æ—Å—Å—ã –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
optimizer = torch.optim.Adam(model.parameters(), lr = CONFIG['lr'])
regression_criterion = nn.MSELoss().to(device)
classification_criterion = nn.CrossEntropyLoss().to(device)
```
Training 5 epochs with windb:
```
import random

num_epochs = CONFIG['epochs']
verbose = 5

for _ in range(5):
    
    # –ò–∑–º–µ–Ω–∏–º CONFIG - –æ–ø—Ä–µ–¥–µ–ª–∏–º dropout –∫–∞–∫ —Ä–∞–Ω–¥–æ–º–Ω—É—é –≤–µ–ª–∏—á–∏–Ω—É –≤ –∑–∞–¥–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ –∑–Ω–∞—á–µ–Ω–∏–π
    CONFIG['dropout'] = random.uniform(0.01, 0.80)
    model = TabularNN(CONFIG).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr = CONFIG['lr'])
    
    # üêù initialize a wandb run
    wandb.init(project='Course contest',
               config=CONFIG,
               group='MLP', 
               job_type='train'
            )
    
    val_acc_history = []

    best_model_wts = copy.deepcopy(model.state_dict())
    best_loss = np.inf

    for epoch in range(num_epochs):
        if epoch % verbose == 0:
            print('Epoch {}/{}'.format(epoch, num_epochs - 1))
            print('-' * 10)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0

            for inputs, labels_1, labels_2 in dataloaders[phase]:
                inputs = inputs.to(device)
                labels_1 = labels_1.to(device)
                labels_2 = labels_2.to(device)
                
                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    # Get model outputs and calculate loss

                    outputs_1, outputs_2 = model(inputs)
                    loss_1 = regression_criterion(outputs_1, labels_1)
                    loss_2 = classification_criterion(outputs_2, labels_2)

                    loss = loss_1 + loss_2

                    _, preds_2 = torch.max(outputs_2, 1)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                val_acc_history.append(running_loss)

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            if epoch % verbose == 0:
                print('{} Loss: {:.4f}'.format(phase, epoch_loss))
            
            # üêù Log train and validation metrics to wandb
            wandb.log({'{} loss'.format(phase): epoch_loss, 'dropout':CONFIG['dropout']})

            # deep copy the model
            if phase == 'val' and epoch_loss < best_loss:
                best_model_wts = copy.deepcopy(model.state_dict())
                best_loss = epoch_loss

    # load best model weights
    model.load_state_dict(best_model_wts)
    
    # üêù Close your wandb run 
    wandb.finish()
```
#### üíé –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞¬†`W&B`.
`W&B Artifacts`¬†–ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∞–º —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –≤—Ö–æ–¥—è—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö) –∏ –≤—ã—Ö–æ–¥—è—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤–µ—Å–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏) –∏–∑ —ç—Ç–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤.

–î—Ä—É–≥–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã - —ç—Ç–æ —Å–ø–æ—Å–æ–± —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–∞—à–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏. –í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å¬†[—ç—Ç–æ—Ç Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W%26B_Artifacts.ipynb), —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ–± –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞—Ö.

##### –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ—é —Ä–∞–±–æ—Ç—É —Å –ø–æ–º–æ—â—å—é¬†`wandb.log_artifact ()`

–í¬†`run`¬†–µ—Å—Ç—å —Ç—Ä–∏ —à–∞–≥–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞ –º–æ–¥–µ–ª–∏.

1. –°–æ–∑–¥–∞–π—Ç–µ –ø—É—Å—Ç–æ–π –∞—Ä—Ç–µ—Ñ–∞–∫—Ç —Å –ø–æ–º–æ—â—å—é¬†`wandb.Artifact ()`.
2. –î–æ–±–∞–≤—å—Ç–µ —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏ –≤ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç —Å –ø–æ–º–æ—â—å—é¬†`wandb.add_file ()`.
3. –í—ã–∑–æ–≤–∏—Ç–µ¬†`wandb.log_artifact ()`, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ê—Ä—Ç–µ—Ñ–∞–∫—Ç.

##### –ü—Ä–∏–º–µ—Ä
```
# Save model
torch.save(model.state_dict(), 'tab_model.pth')

# Initialize a new W&B run
run = wandb.init(project='Course contest',
                 config=CONFIG,
                 group='MLP', 
                 job_type='save') # Note the job_type

# Update `wandb.config`
wandb.config.type = 'baseline'
wandb.config.kaggle_competition = 'Competitive Data Science Course'

# Save model as Model Artifact
artifact = wandb.Artifact(name='best_tab_NN', type='model') # –ó–∞–¥–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–µ –∏–º—è
artifact.add_file('tab_model.pth')
run.log_artifact(artifact)

# Finish W&B run
run.finish()
```
## üé≤ –†–∞–±–æ—Ç–∞ —Å –º–µ—Ç—Ä–∏–∫–æ–π. –ü—Ä–µ-–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∏ –ø–æ—Å—Ç-–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥

1. –ß–µ–∫–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞
2. –ß–µ–∫–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –ø—Ä–µ–¥–∏–∫—Ç–µ
3. –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ - –ø–æ—Ä–æ–∫
4. –ù–µ –æ–∫—Ä—É–≥–ª—è–π, –µ—Å–ª–∏ AUC
	–ú—ã —á–∞—Å—Ç–æ –∑–∞–º–µ—á–∞–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –æ–∫—Ä—É–≥–ª–µ–Ω–∏—è —É –Ω–æ–≤–∏—á–∫–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Å–∏–ª—å–Ω–æ –º–æ–∂–µ—Ç –ø–æ–¥–∫–æ—Å–∏—Ç—å –≤ –Ω–∞—á–∞–ª–µ. –î–æ–ø—É—Å—Ç–∏–º –≤ –≤–∞—à–µ–º —á–µ–º–ø–∏–æ–Ω–∞—Ç–µ –º–µ—Ç—Ä–∏–∫–∞¬†`roc-auc`. ¬†–ï—Å–ª–∏ —É –≤–∞—Å —Ç–µ—Å—Ç–∏—Ä—É—é—â–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –±–æ—Ä–¥–µ¬†`roc-auc`, —Ç–æ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–µ –Ω–∞–¥–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –∫–ª–∞—Å—Å—ã. –≠—Ç–æ –≤—Å–µ–≥–¥–∞ –¥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ö—É–∂–µ, —á–µ–º —Å–∞–º–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏. –ü–æ—ç—Ç–æ–º—É –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ, —á—Ç–æ –≤—ã –∑–∞—Å—ã–ª–∞–µ—Ç–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏!
5. –û–±—Ä–∞—Ç–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏–ª–∏¬†`1 - P`

## ¬†üì∑ Multi-threaded video processing
```
# importing required libraries 
import cv2 
import time 
from threading import Thread # library for implementing multi-threaded processing 

# defining a helper class for implementing multi-threaded processing 
class WebcamStream :
    def __init__(self, stream_id=0):
        self.stream_id = stream_id   # default is 0 for primary camera 
        
        # opening video capture stream 
        self.vcap      = cv2.VideoCapture(self.stream_id)
        if self.vcap.isOpened() is False :
            print("[Exiting]: Error accessing webcam stream.")
            exit(0)
        fps_input_stream = int(self.vcap.get(5))
        print("FPS of webcam hardware/input stream: {}".format(fps_input_stream))
            
        # reading a single frame from vcap stream for initializing 
        self.grabbed , self.frame = self.vcap.read()
        if self.grabbed is False :
            print('[Exiting] No more frames to read')
            exit(0)

        # self.stopped is set to False when frames are being read from self.vcap stream 
        self.stopped = True 

        # reference to the thread for reading next available frame from input stream 
        self.t = Thread(target=self.update, args=())
        self.t.daemon = True # daemon threads keep running in the background while the program is executing 
        
    # method for starting the thread for grabbing next available frame in input stream 
    def start(self):
        self.stopped = False
        self.t.start() 

    # method for reading next frame 
    def update(self):
        while True :
            if self.stopped is True :
                break
            self.grabbed , self.frame = self.vcap.read()
            if self.grabbed is False :
                print('[Exiting] No more frames to read')
                self.stopped = True
                break 
        self.vcap.release()

    # method for returning latest read frame 
    def read(self):
        return self.frame

    # method called to stop reading frames 
    def stop(self):
        self.stopped = True 


# initializing and starting multi-threaded webcam capture input stream 
webcam_stream = WebcamStream(stream_id=0) #  stream_id = 0 is for primary camera 
webcam_stream.start()

# processing frames in input stream
num_frames_processed = 0 
start = time.time()
while True :
    if webcam_stream.stopped is True :
        break
    else :
        frame = webcam_stream.read() 

    # adding a delay for simulating time taken for processing a frame 
    delay = 0.03 # delay value in seconds. so, delay=1 is equivalent to 1 second 
    time.sleep(delay) 
    num_frames_processed += 1 

    cv2.imshow('frame' , frame)
    key = cv2.waitKey(1)
    if key == ord('q'):
        break
end = time.time()
webcam_stream.stop() # stop the webcam stream 

# printing time elapsed and fps 
elapsed = end-start
fps = num_frames_processed/elapsed 
print("FPS: {} , Elapsed Time: {} , Frames Processed: {}".format(fps, elapsed, num_frames_processed))

# closing all windows 
cv2.destroyAllWindows()
```

## ‚ùì‚ùìThings to consider!
- DuckDB : —á–µ—Ä–µ–∑  DuckDB –æ—á–µ–Ω—å –∏–∑—è—â–Ω–æ –≤—ã–¥–µ—Ä–≥–∏–≤–∞–µ—Ç—Å—è –Ω—É–∂–Ω–æ–µ –∏–∑ –Ω–µ–ø–æ–¥—ä–µ–º–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –≤ –ø–∞–º—è—Ç—å –Ω–µ –ª–µ–∑–µ—Ç. –ù–µ –≤–∏–¥–µ–ª —Ç–∞–∫–æ–µ —Ä–∞–Ω—å—à–µ, –ø–æ—Ç–æ–º—É —Ä–∞—Å—à–∞—Ä–∏–ª —Ç—É—Ç
- RAPIDS

## üì∑ Torchvision video processing

[Torchvision](https://github.com/pytorch/vision) implemented a video reader that helps decode videos using the GPU. It is still in Beta stage, and requires some work to set up, as you must build it from source. The steps are pretty straightforward: pre-install FFmpeg, download [Nvidia‚Äôs Video Codec SDK](https://developer.nvidia.com/nvidia-video-codec-sdk), and make sure you‚Äôre building the package with a matching Torch version. The end result is a Torchvision package with GPU video decoding capabilities that‚Äôs super easy to use. You can then take this package and publish it to your local repository, making it accessible to everyone on your team.

```
import torch
import torchvision
 
 
def preprocess_video_gpu(video_path, image_size_low, frames_per_cycle=30):
   reader = torchvision.io.VideoReader(video_path, "video", num_threads=0, device="cuda")
   resizer = torchvision.transforms.Resize(image_size_low, antialias=True)
 
   curr_frames = []
   all_frames_low = []
 
   for frame in reader:
       curr_frames.append(frame["data"])
 
       if len(curr_frames) == frames_per_cycle:
           resize_chunk(curr_frames, all_frames_low, resizer)
           curr_frames = []
 
   if len(curr_frames) > 0:
       resize_chunk(curr_frames, all_frames_low, resizer)
 
   all_frames_low = torch.cat(all_frames_low, 0)
 
   return all_frames_low
 
def resize_chunk(curr_frames, all_frames_low, resizer):
   curr_frames = torch.stack(curr_frames, 0)
   curr_frames = curr_frames.permute(0, 3, 1, 2)
   curr_frames_low = resizer(curr_frames)
   curr_frames_low = curr_frames_low.permute(0, 2, 3, 1)
   all_frames_low.append(curr_frames_low)
 
print("Preprocessing on GPU...")
s = time.time()
frames = preprocess_video_gpu(video_path=VIDEO_PATH, image_size_low=IMAGE_SIZE_LOW)
print("Preprocess on GPU time:", time.time() - s)
```
This code initializes the VideoReader object, then reads frames into a list. Keep in mind, we want to perform the resize on big batches of frames, for better performance. However, holding all the full-size frames in GPU memory might cause us to crash, depending on the video length and the amount of GPU memory available, so we use the frames_per_cycle parameter. Every frames_per_cycle frames, we resize the current frames, and move on to the next batch. Depending on the GPU used, and the video‚Äôs original resolution, we can fine-tune this parameter. I used a T4 GPU



# Making Deep Learning Go Brrrr From First Principles

Just like with training ML models, knowing what regime you're in allows you to narrow in on optimizations that matters. For example, if you're spending all of your time doing memory transfers (i.e. you are in an¬†_memory-bandwidth bound_¬†regime), then increasing the FLOPS of your GPU won't help. On the other hand, if you're spending all of your time performing big chonky matmuls (i.e. a¬†_compute-bound_¬†regime), then rewriting your model logic into C++ to reduce overhead won't help.

So, if you want to keep your GPUs going brrrr, let's discuss the three components your system might be spending time on - compute, memory bandwidth, and overhead.

## Compute

One perspective on optimizing deep learning systems is that we'd like to maximize the time in the compute-bound regime. You paid for all of those 312 teraflops, and ideally, you'd¬†_get_¬†those 312 teraflops. But, in order to get your money's worth out of your expensive matrix multiplication, you need to reduce the amount of time spent in the other parts.

But why the focus on maximizing compute and not say, memory bandwidth? The reason is simple - you can reduce the overhead or memory costs, but you (mostly) can't reduce the computation required without changing the actual operations you're performing.

Exacerbating the difficulty of maximizing compute utilization is the rate at which compute grows compared to memory bandwidth
![[Pasted image 20240614113005.png]]


One way to think about compute is as a factory. We send instructions to our factory (overhead), send it materials (memory-bandwidth), all to keep our factory running efficiently (compute).
So, if our factory increases efficiency faster than the rate at which we can supply it materials, it becomes harder for our factory to achieve its peak efficiency.

¬†If you aren't doing matrix multiplication, you'll only be able to achieve 19.5 teraflops instead of the stated 312. Note that this isn't unique to GPUs - in fact, TPUs are even¬†_less_¬†general than GPUs.
The fact that GPUs are so much slower at everything that isn't a matrix multiply might seem problematic at first - what about our other operators like layer norm or activation functions? Well, the truth is, those operators are just rounding errors in terms of FLOPS. For example, let's look at this table of FLOP counts on BERT for different operator types from¬†[this paper](https://arxiv.org/abs/2007.00072), where "Tensor Contraction" = matmuls.

![[Pasted image 20240614113646.png]]You can see that altogether, our non-matmul ops only make up 0.2% of our FLOPS, so it doesn't matter that our GPU computes non-matmul ops 15x slower.
But, in this case, the normalization and pointwise ops actually achieve¬†**250x less FLOPS and 700x less FLOPS**¬†than our matmuls respectively.

So why do our non-matmul ops take so much more time than they should?

Going back to our analogy, the culprit is often how long it takes to transport materials to and from the factory. In other words, the memory bandwidth.

## Bandwidth

Bandwidth costs are essentially the cost paid to move data from one place to another. This might be moving the data from CPU to GPU, from one node to another, or even from CUDA global memory to CUDA shared memory. This last one, in particular, is what we'll be focusing on here, and is **typically referred to as "bandwidth cost" or "memory bandwidth cost".**

Although our factory is where we do the actual work, it's not suitable as a bulk storage unit. A large part of this is that since we're doing actual work here, all the storage is optimized for being fast to actually¬†_use_¬†(SRAM), instead of having a lot of it.

So, where do we store the actual results and materials? The typical approach is to have a warehouse, probably somewhere where land is cheap and we have a lot of space (DRAM). Then, we can ship supplies to and from our factories (memory bandwidth).

As an aside, your GPU's DRAM is what shows up in¬†`nvidia-smi`, and is the primary quantity responsible for your lovely "CUDA Out of Memory' errors.

Now, imagine what happens when we perform an unary operation like¬†`torch.cos`. We need to ship our data from our storage to the warehouse, then perform a tiny bit of computation for each piece of data, and then ship that storage back. Shipping things around is quite expensive. As a result, nearly all of our time here is spent shipping data around, and¬†_not_¬†on the actual computation itself.

Since we're spending all of our time on memory-bandwidth, such an operation is called a¬†**memory-bound operation**, and it means that we're not spending a lot of time on compute.

operator fusion - the most important optimization in deep learning compilers. Simply put, instead of writing our data to global memory just to read it again, we elide the extra memory accesses by performing several computations at once.

For example, if we perform¬†`x.cos().cos()`, usually we need to perform 4 global reads and writes.

```
x1 = x.cos() # Read from x in global memory, write to x1
x2 = x1.cos() # Read from x1 in global memory, write to x2
```
But, with operator fusion, we only need 2 global memory reads and writes! So operator fusion will speed it up by 2x.
```
x2 = x.cos().cos() # Read from x in global memory, write to x2
```

Not all operator fusion is as simple as pointwise operators. You can fuse pointwise operators onto reductions, or pointwise operators onto matrix multiplication. Even matrix multiplication itself can be thought of as fusing a broadcasting multiply followed by a reduction.

Finally, operator fusion leads to some surprising consequences. For one, a fused¬†`x.cos().cos()`¬†will take nearly the exact same time as calling¬†`x.cos()`¬†by itself. This is why activation functions are nearly all the same cost, despite¬†`gelu`¬†obviously consisting of many more operations than¬†`relu`.

This fact leads to some interesting consequences for rematerialization/activation checkpointing. Essentially, doing extra recomputation might lead to¬†_less_¬†memory-bandwidth, and thus less runtime. Thus, we can lower both memory¬†_and_¬†runtime through rematerialization, which we leveraged to build a neat min-cut optimization pass in AOTAutograd. You can read more about it¬†[here](https://dev-discuss.pytorch.org/t/min-cut-optimal-recomputation-i-e-activation-checkpointing-with-aotautograd/467/1)

## Reasoning about Memory-Bandwidth costs

When it come to reasoning about whether your operation is memory-bandwidth bound, a calculator can go a long way.

For simple operators, it's feasible to reason about your memory bandwidth directly. For example, an A100 has 1.5 terabytes/second of global memory bandwidth, and can perform 19.5 teraflops/second of compute. So, if you're using 32 bit floats (i.e. 4 bytes), you can load in 400 billion numbers in the same time that the GPU can perform 20 trillion operations. Moreover, to perform a simple unary operator (like multiplying a tensor by 2), we actually need to¬†_write_¬†the tensor back to global memory.

So... until you're doing about a hundred operations in your unary operator, you'll be spending more time performing memory accesses than actual compute.

With the help of a fusing compiler like NVFuser, it's actually fairly easy to measure this ourselves! You can see the code in Colab¬†[here](https://colab.research.google.com/drive/1hEtorT5y9mcXHR0gpensD7oZfuyyxtu7?usp=sharing).

If you take a PyTorch function like
```
def f(x: Tensor[N]):
    for _ in range(repeat):
        x = x * 2
    return x
```

and benchmark it with a fusing compiler, we can then calculate the FLOPS and memory bandwidth achieved for various values of¬†`repeat`. Increasing¬†`repeat`¬†is an easy way of increasing our amount of compute¬†_without_¬†increasing our memory accesses - this is also known as increasing¬†**compute intensity**.

## Overhead
Overhead is when your code is spending time doing anything that's¬†**not**¬†transferring tensors or computing things. For example, time spent in the Python interpreter? Overhead. Time spent in the PyTorch framework? Overhead. Time spent launching CUDA kernels (but not executing them)? Also overhead.

The primary reason overhead is such a pernicious problem is that modern GPUs are¬†_really_¬†fast. An A100 can perform 312¬†**trillion**¬†floating point operations per second (312 TeraFLOPS). In comparison, Python is¬†_really_¬†slooooowwww. Benchmarking locally, Python can perform 32 million additions in one second.

That means that in the time that Python can perform a¬†_single_¬†FLOP, an A100 could have chewed through¬†**9.75 million FLOPS**.

Even worse, the Python interpreter isn't even the only source of overhead - frameworks like PyTorch also have many layers of dispatch before you get to your actual kernel. If you perform the same experiment with PyTorch, we can only get 280 thousand operations per second. Of course, tiny tensors aren't what PyTorch is built for, but... if you are using tiny tensors (such as in scientific computing), you might find PyTorch incredibly slow compared to C++.

Given this, you might be shocked that anybody uses PyTorch at all, but keep in mind that modern deep learning models are often performing¬†**massive**¬†operations. Moreover, frameworks like PyTorch execute¬†_asynchronously_. That is, while PyTorch is running a CUDA kernel, it can continue and queue up more CUDA kernels behind it. So, as long as PyTorch can "run ahead" of the CUDA kernels, most of the framework overhead gets completely hidden!

**So, how do you tell if you're in this regime? Well, since overhead generally doesn't scale with problem size (while compute and memory do), the easiest way to tell is to simply increase the size of your data. If that doesn't increase the runtime proportionally, you're overhead bound. For example, if you double your batch size but your runtime only increases by 10%, you're likely overhead bound.**

Another aside - the "GPU-Util" ([not "Volatile GPU-Util"](https://twitter.com/cHHillee/status/1500547396945670144)) entry in nvidia-smi is basically measuring what percentage of the bottom row is actually running a GPU kernel. So that's another good way of eyeballing overhead.

Fundamentally, this overhead comes from the flexibility of being able to do something different at each step. If you don't need this flexibility, one way of resolving this flexibility is by tracing it out, like with¬†`jit.trace`,¬†`FX`, or¬†`jax.jit`. Or, alternately, you could do it at an even lower level with something like¬†[CUDA Graphs](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/).

## Conclusion
